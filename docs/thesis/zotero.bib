@online{microsoft_net_2025,
	title = {.{NET} Coding Conventions - C\#},
	url = {https://learn.microsoft.com/en-us/dotnet/csharp/fundamentals/coding-style/coding-conventions},
	abstract = {Learn about commonly used coding conventions in C\#. Coding conventions create a consistent look to the code and facilitate copying, changing, and maintaining the code. This article also includes the docs repo coding guidelines},
	author = {{Microsoft}},
	urldate = {2025-09-08},
	date = {2025},
	langid = {english},
}

@article{sobania_comprehensive_2023,
	title = {A Comprehensive Survey on Program Synthesis With Evolutionary Algorithms},
	volume = {27},
	issn = {1941-0026},
	url = {https://ieeexplore.ieee.org/document/9743417},
	doi = {10.1109/TEVC.2022.3162324},
	abstract = {The automatic generation of computer programs is one of the main applications with practical relevance in the field of evolutionary computation. With program synthesis techniques not only software developers could be supported in their everyday work but even users without any programming knowledge could be empowered to automate repetitive tasks and implement their own new functionality. In recent years, many novel program synthesis approaches based on evolutionary algorithms have been proposed and evaluated on common benchmark problems. Therefore, we identify and discuss in this survey the relevant evolutionary program synthesis approaches in the literature and provide an in-depth analysis of their performance. The most influential approaches we identify are stack-based, grammar-guided, as well as linear genetic programming ({GP}). For the stack-based approaches, we identify 37 in-scope papers, and for the grammar-guided and linear {GP} approaches, we identify 12 and 5 papers, respectively. Furthermore, we find that these approaches perform well on benchmark problems if there is a simple mapping from the given input to the correct output. On problems where this mapping is complex, e.g., if the problem consists of several subproblems or requires iteration/recursion for a correct solution, results tend to be worse. Consequently, for future work, we encourage researchers not only to use a program’s output for assessing the quality of a solution but also the way toward a solution (e.g., correctly solved subproblems).},
	pages = {82--97},
	number = {1},
	journaltitle = {{IEEE} Transactions on Evolutionary Computation},
	author = {Sobania, Dominik and Schweim, Dirk and Rothlauf, Franz},
	urldate = {2025-01-13},
	date = {2023-02},
	note = {Conference Name: {IEEE} Transactions on Evolutionary Computation},
	keywords = {Benchmark testing, Benchmarks, Codes, Evolutionary computation, Natural languages, Programming, Python, Task analysis, evolutionary algorithms, genetic programming ({GP}), program synthesis},
}

@article{yang_control-theoretic_2012,
	title = {A control-theoretic study on iterative solutions to nonlinear equations for applications in embedded systems},
	volume = {48},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109812000210},
	doi = {10.1016/j.automatica.2012.01.007},
	abstract = {In this paper, the fixed point iteration and Newton’s methods for iteratively solving nonlinear equations are studied in the control theoretical framework. This work is motivated by the ever increasing demands for integrating iterative solutions of nonlinear functions into embedded control systems. The use of the well-established control theoretical methods for our application purpose is inspired by the recent control-theoretical study on numerical analysis. Our study consists of two parts. In the first part, the existing fixed point iteration and Newton’s methods are analysed using the stability theory for the sector-bounded Lure’s systems. The second part is devoted to the modified iteration methods and the integration of sensor signals into the iterative computations. The major results achieved in our study are, besides some academic examples, applied to the iterative computation of the air path model embedded in the engine control systems.},
	pages = {583--594},
	number = {4},
	journaltitle = {Automatica},
	shortjournal = {Automatica},
	author = {Yang, Ying and Ding, Steven X.},
	urldate = {2025-02-04},
	date = {2012-04-01},
	keywords = {Fixed point iteration, {LMI} technique, Newton’s methods, Numerical analysis, Observer design, Stability theory},
}

@inproceedings{aivaloglou_grammar_2015,
	location = {Bremen, Germany},
	title = {A grammar for spreadsheet formulas evaluated on two large datasets},
	isbn = {978-1-4673-7529-0},
	url = {http://ieeexplore.ieee.org/document/7335408/},
	doi = {10.1109/SCAM.2015.7335408},
	abstract = {Spreadsheets are ubiquitous in the industrial world and often perform a role similar to other computer programs, which makes them interesting research targets. However, there does not exist a reliable grammar that is concise enough to facilitate formula parsing and analysis and to support research on spreadsheet codebases.},
	eventtitle = {2015 {IEEE} 15th International Working Conference on Source Code Analysis and Manipulation ({SCAM})},
	pages = {121--130},
	booktitle = {2015 {IEEE} 15th International Working Conference on Source Code Analysis and Manipulation ({SCAM})},
	publisher = {{IEEE}},
	author = {Aivaloglou, Efthimia and Hoepelman, David and Hermans, Felienne},
	urldate = {2025-07-04},
	date = {2015-09},
	langid = {english},
}

@article{rothermel_methodology_2001,
	title = {A methodology for testing spreadsheets},
	volume = {10},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/366378.366385},
	doi = {10.1145/366378.366385},
	abstract = {Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults; thus, we would like to provide at least some of the benefits of formal testing methodologies to the creators of spreadsheets. This article presents a testing methodology that adapts data flow adequacy criteria and coverage monitoring to the task of testing spreadsheets. To accommodate the evaluation model used with spreadsheets, and the interactive process by which they are created, our methodology is incremental. To accommodate the users of spreadsheet languages, we provide an interface to our methodology that does not require an understanding of testing theory. We have  implemented our testing methodology in the context of the Forms/3 visual spreadsheet language. We report  on the methodology, its time and space costs, and the mapping from the testing strategy to the user interface. In an empirical study, we found that test suites created according to our methodology detected, on average, 81\% of the faults in a set of faulty spreadsheets, significantly outperforming randomly generated test suites.},
	pages = {110--147},
	number = {1},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Rothermel, Gregg and Burnett, Margaret and Li, Lixin and Dupuis, Christopher and Sheretov, Andrei},
	urldate = {2025-01-27},
	date = {2001-01},
	langid = {english},
}

@article{khan_picard-mann_2013,
	title = {A Picard-Mann hybrid iterative process},
	volume = {2013},
	issn = {1687-1812},
	url = {https://doi.org/10.1186/1687-1812-2013-69},
	doi = {10.1186/1687-1812-2013-69},
	abstract = {We introduce a new iterative process which can be seen as a hybrid of Picard and Mann iterative processes. We show that the new process converges faster than all of Picard, Mann and Ishikawa iterative processes in the sense of Berinde (Iterative Approximation of Fixed Points, 2002) for contractions. We support our analytical proof by a numerical example. We prove a strong convergence theorem with the help of our process for the class of nonexpansive mappings in general Banach spaces and apply it to get a result in uniformly convex Banach spaces. Our weak convergence results are proved when the underlying space satisfies Opial’s condition or has Fréchet differentiable norm or its dual satisfies the Kadec-Klee property.},
	pages = {69},
	number = {1},
	journaltitle = {Fixed Point Theory and Applications},
	shortjournal = {Fixed Point Theory and Applications},
	author = {Khan, Safeer Hussain},
	urldate = {2025-02-05},
	date = {2013-03-25},
	keywords = {contraction, fixed point, iterative process, nonexpansive mapping, rate of convergence, strong convergence, weak convergence},
}

@inproceedings{rebaudengo_source--source_2001,
	title = {A source-to-source compiler for generating dependable software},
	url = {https://ieeexplore.ieee.org/abstract/document/972664},
	doi = {10.1109/SCAM.2001.972664},
	abstract = {Over the last years, an increasing number of safety-critical tasks have been demanded for computer systems. In particular, safety-critical computer-based applications are hitting market areas where cost is a major issue, and thus solutions are required which conjugate fault tolerance with low costs. A source-to-source compiler supporting a software-implemented hardware fault tolerance approach is proposed, based on a set of source code transformation rules. The proposed approach hardens a program against transient memory errors by introducing software redundancy: every computation is performed twice and results are compared, and control flow invariants are checked explicitly. By exploiting the tool's capabilities, several benchmark applications have been hardened against transient errors. Fault injection campaigns have been performed to evaluate the fault detection capability of the hardened applications. In addition, we analyzed the proposed approach in terms of space and time overheads.},
	eventtitle = {First {IEEE} International Workshop on Source Code Analysis and Manipulation},
	pages = {33--42},
	booktitle = {Proceedings First {IEEE} International Workshop on Source Code Analysis and Manipulation},
	author = {Rebaudengo, M. and Reorda, M.S. and Violante, M. and Torchiano, M.},
	urldate = {2025-07-13},
	date = {2001-11},
	keywords = {Application software, Computer applications, Costs, Error correction, Fault detection, Fault tolerance, Hardware, Performance evaluation, Redundancy, Software performance},
}

@book{sestoft_spreadsheet_2006,
	location = {Copenhagen, Denmark},
	edition = {Version 1.0 of 2006-09-28},
	title = {A spreadsheet core implementation in C\#},
	isbn = {978-87-7949-135-9},
	series = {{IT} University technical report series / {IT} University of Copenhagen},
	pagetotal = {1},
	number = {{TR}-91 (2006)},
	publisher = {{IT} University of Copenhagen},
	author = {Sestoft, Peter},
	date = {2006},
	langid = {english},
}

@article{burstall_transformation_1977,
	title = {A Transformation System for Developing Recursive Programs},
	volume = {24},
	doi = {10.1145/321992.321996},
	abstract = {A system of rules for transforming programs is described, with the programs in the form of recursion equations. An initially very simple, lucid, and hopefully correct program is transformed into a more efficient one by altering the recursion structure. Illustrative examples of program transformations are given, and a tentative implementation is described. Alternative structures for programs are shown, and a possible initial phase for an automatic or semiautomatic program-manipulation system is indicated.},
	pages = {44--67},
	number = {1},
	journaltitle = {Journal of the {ACM}},
	author = {Burstall, Rod M. and Darlington, John},
	date = {1977-01-01},
	doi = {10.1145/321992.321996},
	note = {{MAG} {ID}: 2023299380
S2ID: f54a584f23c0940a6314c3782dd59b9893b8ae0a},
}

@inproceedings{lano_agile_2017,
	title = {Agile Model-driven Engineering of Financial Applications.},
	url = {https://nms.kcl.ac.uk/kevin.lano/finmdd/flexmde17f.pdf},
	pages = {388--392},
	booktitle = {{MoDELS} (Satellite Events)},
	author = {Lano, Kevin and Haughton, Howard P. and Tehrani, Sobhan Yassipour and Alfraihi, Hessa},
	urldate = {2025-01-13},
	date = {2017},
}

@article{winter_agile_2014,
	title = {Agile Software Development: Principles, Patterns, and Practices: Robert C. Martin with contributions by James W. Newkirk and Robert S. Koss},
	volume = {53},
	rights = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	issn = {10908811},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/pfi.21408},
	doi = {10.1002/pfi.21408},
	shorttitle = {Agile Software Development},
	pages = {43--46},
	number = {4},
	journaltitle = {Performance Improvement},
	shortjournal = {Perf. Improv.},
	author = {Winter, Robert J.},
	urldate = {2025-09-03},
	date = {2014-04},
	langid = {english},
}

@software{apache_software_apache_nodate,
	title = {Apache {POI}},
	url = {https://poi.apache.org/components/spreadsheet/eval.html},
	author = {{Apache Software}},
}

@article{ordonez_camacho_automated_2010,
	title = {Automated generation of program translation and verification tools using annotated grammars},
	volume = {75},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01676423},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642309001324},
	doi = {10.1016/j.scico.2009.10.003},
	abstract = {Automatically generating program translators from source and target language specifications is a non-trivial problem. In this paper we focus on the problem of automating the process of building translators between operations languages, a family of {DSLs} used to program satellite operations procedures. We exploit their similarities to semi-automatically build transformation tools between these {DSLs}. The input to our method is a collection of annotated context-free grammars. To simplify the overall translation process even more, we also propose an intermediate representation common to all operations languages. Finally, we discuss how to enrich our annotated grammars model with more advanced semantic annotations to provide a verification system for the translation process. We validate our approach by semi-automatically deriving translators between some real world operations languages, using the prototype tool which we implemented for that purpose.},
	pages = {3--20},
	number = {1},
	journaltitle = {Science of Computer Programming},
	shortjournal = {Science of Computer Programming},
	author = {Ordóñez Camacho, Diego and Mens, Kim and Brand, Mark Van Den and Vinju, Jurgen},
	urldate = {2025-01-29},
	date = {2010-01},
	langid = {english},
}

@inproceedings{fisher_automated_2002,
	title = {Automated test case generation for spreadsheets},
	url = {https://ieeexplore.ieee.org/document/1007963},
	doi = {10.1145/581356.581359},
	abstract = {Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults. Thus, in previous work, we presented a methodology that assists spreadsheet users in testing their spreadsheet formulas. Our empirical studies have shown that this methodology can help end-users test spreadsheets more adequately and efficiently; however, the process of generating test cases can still represent a significant impediment. To address this problem, we have been investigating how to automate test case generation for spreadsheets in ways that support incremental testing and provide immediate visual feedback. We have utilized two techniques for generating test cases, one involving random selection and one involving a goal-oriented approach. We describe these techniques, and report results of an experiment examining their relative costs and benefits.},
	eventtitle = {Proceedings of the 24th International Conference on Software Engineering. {ICSE} 2002},
	pages = {141--151},
	booktitle = {Proceedings of the 24th International Conference on Software Engineering. {ICSE} 2002},
	author = {Fisher, M. and Cao, Mingming and Rothermel, G. and Cook, C.R. and Burnett, M.M.},
	urldate = {2025-01-27},
	date = {2002-05},
	keywords = {Automatic testing, Business, Computer aided software engineering, Computer science, Impedance, Output feedback, Permission, Programming profession, Software engineering, Tail},
}

@inproceedings{cunha_automatically_2010,
	title = {Automatically Inferring {ClassSheet} Models from Spreadsheets},
	url = {https://ieeexplore.ieee.org/document/5635202},
	doi = {10.1109/VLHCC.2010.22},
	abstract = {Many errors in spreadsheet formulas can be avoided if spreadsheets are built automatically from higher-level models that can encode and enforce consistency constraints. However, designing such models is time consuming and requires expertise beyond the knowledge to work with spreadsheets. Legacy spreadsheets pose a particular challenge to the approach of controlling spreadsheet evolution through higher-level models, because the need for a model might be overshadowed by two problems: (A) The benefit of creating a spreadsheet is lacking since the legacy spreadsheet already exists, and (B) existing data must be transferred into the new model-generated spreadsheet. To address these problems and to support the model-driven spreadsheet engineering approach, we have developed a tool that can automatically infer {ClassSheet} models from spreadsheets. To this end, we have adapted a method to infer entity/relationship models from relational database to the spreadsheets/{ClassSheets} realm. We have implemented our techniques in the {HaExcel} framework and integrated it with the {ViTSL}/Gencel spreadsheet generator, which allows the automatic generation of refactored spreadsheets from the inferred {ClassSheet} model. The resulting spreadsheet guides further changes and provably safeguards the spreadsheet against a large class of formula errors. The developed tool is a significant contribution to spreadsheet (reverse) engineering, because it fills an important gap and allows a promising design method ({ClassSheets}) to be applied to a huge collection of legacy spreadsheets with minimal effort.},
	eventtitle = {2010 {IEEE} Symposium on Visual Languages and Human-Centric Computing},
	pages = {93--100},
	booktitle = {2010 {IEEE} Symposium on Visual Languages and Human-Centric Computing},
	author = {Cunha, Jacome and Erwig, Martin and Saraiva, Joao},
	urldate = {2025-01-13},
	date = {2010-09},
	note = {{ISSN}: 1943-6106},
	keywords = {Business, {ClassSheets} inference, Context, Data models, Marketing and sales, Object oriented modeling, Programming, Spreadsheets, Unified modeling language},
}

@inproceedings{weisz_better_2022,
	location = {New York, {NY}, {USA}},
	title = {Better Together? An Evaluation of {AI}-Supported Code Translation},
	isbn = {978-1-4503-9144-3},
	url = {https://dl.acm.org/doi/10.1145/3490099.3511157},
	doi = {10.1145/3490099.3511157},
	series = {{IUI} '22},
	shorttitle = {Better Together?},
	abstract = {Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced code with fewer errors than when working alone. We also examined how the quality and quantity of {AI} translations affected the work process and quality of outcomes, and observed that providing multiple translations had a larger impact on the translation process than varying the quality of provided translations. Our results tell a complex, nuanced story about the benefits of generative code models and the challenges software engineers face when working with their outputs. Our work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.},
	pages = {369--391},
	booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
	publisher = {Association for Computing Machinery},
	author = {Weisz, Justin D. and Muller, Michael and Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Agarwal, Mayank and Talamadupula, Kartik and Richards, John T.},
	urldate = {2025-01-29},
	date = {2022-03-22},
}

@inproceedings{lopes_chomsky_2005,
	location = {Berlin, Heidelberg},
	title = {Chomsky: A Content Language Translation Agent},
	isbn = {978-3-540-31731-9},
	doi = {10.1007/11559221_54},
	shorttitle = {Chomsky},
	abstract = {This paper describes Chomsky, a content language translation agent. This agent provides a service, which first translates the content expression from its original language into an abstract logic language ({ALL}). Then, the resulting {ALL} expression is translated to the desired target language. {ALL} has been designed as a superset of most known content languages to avoid loosing expressiveness during translation. For more than three supported languages, using an intermediate language approach involves fewer translators than a pair wise approach. Currently, Chomsky supports {FIPA}-{SL}, {KIF}, and Prolog Content Language. In one mode of operation, Chomsky returns the result of the translation request to the client agent. In the other mode, Chomsky receives a message from the intended sender whose content is expressed in language L1, it translates the content to the content language used by the intended receiver (L2), and it sends the message with the L2 content to the intended receiver.},
	pages = {535--538},
	booktitle = {Multi-Agent Systems and Applications {IV}},
	publisher = {Springer},
	author = {Lopes, António and Botelho, Luís},
	editor = {Pěchouček, Michael and Petta, Paolo and Varga, László Zsolt},
	date = {2005},
	langid = {english},
}

@inproceedings{engels_classsheets_2005,
	location = {New York, {NY}, {USA}},
	title = {{ClassSheets}: automatic generation of spreadsheet applications from object-oriented specifications},
	isbn = {978-1-58113-993-8},
	url = {https://doi.org/10.1145/1101908.1101929},
	doi = {10.1145/1101908.1101929},
	series = {{ASE} '05},
	shorttitle = {{ClassSheets}},
	abstract = {Spreadsheets are widely used in all kinds of business applications. Numerous studies have shown that they contain many errors that sometimes have dramatic impacts. One reason for this situation is the low-level, cell-oriented development process of spreadsheets.We improve this process by introducing and formalizing a higher-level object-oriented model termed {ClassSheet}. While still following the tabular look-and feel of spreadsheets, {ClassSheets} allow the developer to express explicitly business object structures within a spreadsheet, which is achieved by integrating concepts from the {UML} (Unified Modeling Language). A stepwise automatic transformation process generates a spreadsheet application that is consistent with the {ClassSheet} model. Thus, by deploying the formal underpinning of {ClassSheets}, a large variety of errors can be prevented that occur in many existing spreadsheet applications today.The presented {ClassSheet} approach links spreadsheet applications to the object-oriented modeling world and advocates an automatic model-driven development process for spreadsheet applications of high quality.},
	pages = {124--133},
	booktitle = {Proceedings of the 20th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Engels, Gregor and Erwig, Martin},
	urldate = {2025-01-26},
	date = {2005-11-07},
}

@article{kovacs_closure-free_2024,
	title = {Closure-Free Functional Programming in a Two-Level Type Theory},
	volume = {8},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3674648},
	doi = {10.1145/3674648},
	abstract = {Many abstraction tools in functional programming rely heavily on general-purpose        compiler optimization to achieve adequate performance. For example, monadic        binding is a higher-order function which yields runtime closures in the absence        of sufficient compile-time inlining and beta-reductions, thereby significantly        degrading performance. In current systems such as the Glasgow Haskell Compiler,        there is no strong guarantee that general-purpose optimization can eliminate        abstraction overheads, and users only have indirect and fragile control over        code generation through inlining directives and compiler options. We propose a        two-stage language to simultaneously get strong guarantees about code generation        and strong abstraction features. The object language is a simply-typed        first-order language which can be compiled without runtime closures. The        compile-time language is a dependent type theory. The two are integrated in a        two-level type theory.                We demonstrate two applications of the system. First, we develop monads and        monad transformers. Here, abstraction overheads are eliminated by staging and we        can reuse almost all definitions from the existing Haskell ecosystem. Second,        we develop pull-based stream fusion. Here we make essential use of dependent        types to give a concise definition of a
              {concatMap}
              operation with        guaranteed fusion. We provide an Agda implementation and a typed Template        Haskell implementation of these developments.},
	pages = {659--692},
	issue = {{ICFP}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Kovács, András},
	urldate = {2025-01-30},
	date = {2024-08-15},
	langid = {english},
}

@book{mcconnell_code_2004,
	location = {Redmond, Wash},
	edition = {2nd ed},
	title = {Code complete},
	isbn = {978-0-7356-1967-8},
	pagetotal = {914},
	publisher = {Microsoft Press},
	author = {{McConnell}, Steve},
	date = {2004},
	langid = {english},
	keywords = {Computer software, Development, Handbooks, manuals, etc},
}

@article{szafraniec_code_2023,
	title = {Code Translation with Compiler Representations},
	url = {http://arxiv.org/abs/2207.03578},
	doi = {10.48550/arXiv.2207.03578},
	abstract = {In this paper, we leverage low-level compiler intermediate representations ({IR}) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation ({NMT}) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of {NMT}, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with {IRs}, specifically {LLVM} {IR}, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11\% on average, and up to 79\% for the Java -{\textgreater} Rust pair with greedy decoding. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of {IR} decompilation, generating programming source code from {IR}, and study using {IRs} as intermediary pivot for translation.},
	author = {Szafraniec, Marc and Roziere, Baptiste and Leather, Hugh and Charton, Francois and Labatut, Patrick and Synnaeve, Gabriel},
	urldate = {2025-01-27},
	date = {2023-04-24},
	eprinttype = {arxiv},
	eprint = {2207.03578 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@inproceedings{szafraniec_code_2022,
	title = {Code Translation with Compiler Representations},
	url = {https://openreview.net/forum?id=XomEU3eNeSQ},
	abstract = {In this paper, we leverage low-level compiler intermediate representations ({IR}) code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation ({NMT}) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of {NMT}, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with {IRs}, specifically {LLVM} {IR}, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11\% on average, and up to 79\% for the Java → Rust pair with greedy decoding. With beam search, it increases the number of correct translations by 5.5\% in average. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of {IR} decompilation, generating programming source code from {IR}, and study using {IRs} as intermediary pivot for translation.},
	eventtitle = {The Eleventh International Conference on Learning Representations},
	author = {Szafraniec, Marc and Roziere, Baptiste and Leather, Hugh James and Labatut, Patrick and Charton, Francois and Synnaeve, Gabriel},
	urldate = {2025-01-29},
	date = {2022-09-29},
	langid = {english},
}

@article{elliott_compiling_2003,
	title = {Compiling embedded languages},
	volume = {13},
	issn = {1469-7653, 0956-7968},
	url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/compiling-embedded-languages/4B0A7526CC16907F445CCF27277E9B9B},
	doi = {10.1017/S0956796802004574},
	abstract = {Functional languages are particularly well-suited to the interpretive implementations of Domain-Specific Embedded Languages ({DSELs}). We describe an implemented technique for producing optimizing compilers for {DSELs}, based on Kamin's idea of {DSELs} for program generation. The technique uses a data type of syntax for basic types, a set of smart constructors that perform rewriting over those types, some code motion transformations, and a back-end code generator. Domain-specific optimization results from chains of domain-independent rewrites on basic types. New {DSELs} are defined directly in terms of the basic syntactic types, plus host language functions and tuples. This definition style makes compilers easy to write and, in fact, almost identical to the simplest embedded interpreters. We illustrate this technique with a language Pan for the computationally intensive domain of image synthesis and manipulation.},
	pages = {455--481},
	number = {3},
	journaltitle = {Journal of Functional Programming},
	author = {Elliott, Conal and Finne, Sigbjørn and Moor, Oege De},
	urldate = {2025-01-27},
	date = {2003-05},
	langid = {english},
}

@incollection{mclennan_computing_2018,
	title = {Computing Fixed Points},
	isbn = {978-981-13-0710-2},
	url = {https://link.springer.com/chapter/10.1007/978-981-13-0710-2_3},
	abstract = {If there are points in the domain of a continuous function from a compact, convex subset of a Euclidean space to itself that are arbitrarily close to their images, the limit of a suitable convergent sequence is a fixed point. The existence of a convergent sequence...},
	pages = {55--101},
	booktitle = {Advanced Fixed Point Theory for Economics},
	publisher = {Springer, Singapore},
	author = {{McLennan}, Andrew},
	urldate = {2025-01-30},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-981-13-0710-2_3},
}

@inproceedings{roy_deeptc-enhancer_2020,
	location = {Virtual Event Australia},
	title = {{DeepTC}-enhancer: improving the readability of automatically generated tests},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416622},
	doi = {10.1145/3324884.3416622},
	shorttitle = {{DeepTC}-enhancer},
	eventtitle = {{ASE} '20: 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {287--298},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Roy, Devjeet and Zhang, Ziyi and Ma, Maggie and Arnaoudova, Venera and Panichella, Annibale and Panichella, Sebastiano and Gonzalez, Danielle and Mirakhorli, Mehdi},
	urldate = {2025-01-25},
	date = {2020-12-21},
	langid = {english},
}

@article{sujeeth_delite_2014,
	title = {Delite: A Compiler Architecture for Performance-Oriented Embedded Domain-Specific Languages},
	volume = {13},
	issn = {1539-9087, 1558-3465},
	url = {https://dl.acm.org/doi/10.1145/2584665},
	doi = {10.1145/2584665},
	shorttitle = {Delite},
	abstract = {Developing high-performance software is a difficult task that requires the use of low-level, architecture-specific programming models (e.g., {OpenMP} for {CMPs}, {CUDA} for {GPUs}, {MPI} for clusters). It is typically not possible to write a single application that can run efficiently in different environments, leading to multiple versions and increased complexity. Domain-Specific Languages ({DSLs}) are a promising avenue to enable programmers to use high-level abstractions and still achieve good performance on a variety of hardware. This is possible because {DSLs} have higher-level semantics and restrictions than general-purpose languages, so {DSL} compilers can perform higher-level optimization and translation. However, the cost of developing performance-oriented {DSLs} is a substantial roadblock to their development and adoption. In this article, we present an overview of the Delite compiler framework and the {DSLs} that have been developed with it. Delite simplifies the process of {DSL} development by providing common components, like parallel patterns, optimizations, and code generators, that can be reused in {DSL} implementations. Delite {DSLs} are embedded in Scala, a general-purpose programming language, but use metaprogramming to construct an Intermediate Representation ({IR}) of user programs and compile to multiple languages (including C++, {CUDA}, and {OpenCL}). {DSL} programs are automatically parallelized and different parts of the application can run simultaneously on {CPUs} and {GPUs}. We present Delite {DSLs} for machine learning, data querying, graph analysis, and scientific computing and show that they all achieve performance competitive to or exceeding C++ code.},
	pages = {1--25},
	number = {4},
	journaltitle = {{ACM} Transactions on Embedded Computing Systems},
	shortjournal = {{ACM} Trans. Embed. Comput. Syst.},
	author = {Sujeeth, Arvind K. and Brown, Kevin J. and Lee, Hyoukjoong and Rompf, Tiark and Chafi, Hassan and Odersky, Martin and Olukotun, Kunle},
	urldate = {2025-01-27},
	date = {2014-07},
	langid = {english},
}

@article{borstler_developers_2023,
	title = {Developers talking about code quality},
	volume = {28},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-023-10381-0},
	doi = {10.1007/s10664-023-10381-0},
	abstract = {There are many aspects of code quality, some of which are difﬁcult to capture or to measure. Despite the importance of software quality, there is a lack of commonly accepted measures or indicators for code quality that can be linked to quality attributes. We investigate software developers’ perceptions of source code quality and the practices they recommend to achieve these qualities. We analyze data from semi-structured interviews with 34 professional software developers, programming teachers and students from Europe and the U.S. For the interviews, participants were asked to bring code examples to exemplify what they consider good and bad code, respectively. Readability and structure were used most commonly as deﬁning properties for quality code. Together with documentation, they were also suggested as the most common target properties for quality improvement. When discussing actual code, developers focused on structure, comprehensibility and readability as quality properties. When analyzing relationships between properties, the most commonly talked about target property was comprehensibility. Documentation, structure and readability were named most frequently as source properties to achieve good comprehensibility. Some of the most important source code properties contributing to code quality as perceived by developers lack clear deﬁnitions and are difﬁcult to capture. More research is therefore necessary to measure the structure, comprehensibility and readability of code in ways that matter for developers and to relate these measures of code structure, comprehensibility and readability to common software quality attributes.},
	pages = {128},
	number = {6},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Börstler, Jürgen and Bennin, Kwabena E. and Hooshangi, Sara and Jeuring, Johan and Keuning, Hieke and Kleiner, Carsten and {MacKellar}, Bonnie and Duran, Rodrigo and Störrle, Harald and Toll, Daniel and Van Assema, Jelle},
	urldate = {2025-09-03},
	date = {2023-11},
	langid = {english},
}

@article{hopcroft_efficient_1973,
	title = {Efficient algorithms for graph manipulation},
	volume = {16},
	rights = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/362248.362272},
	doi = {10.1145/362248.362272},
	shorttitle = {Algorithm 447},
	abstract = {Efficient algorithms are presented for partitioning a graph into connected components, biconnected components and simple paths. The algorithm for partitioning of a graph into simple paths is iterative and each iteration produces a new path between two vertices already on paths. (The start vertex can be specified dynamically.) If V is the number of vertices and E is the number of edges, each algorithm requires time and space proportional to max (V, E) when executed on a random access computer.},
	pages = {372--378},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Hopcroft, John and Tarjan, Robert},
	urldate = {2025-07-14},
	date = {1973-06},
	langid = {english},
	note = {Publisher: Association for Computing Machinery ({ACM})},
}

@software{epplus_software_epplus_nodate,
	title = {{EPPlus}},
	url = {https://github.com/EPPlusSoftware/EPPlus/wiki/Formula-Calculation},
	author = {{EPPlus Software}},
}

@software{aspose_espose_nodate,
	title = {Espose Cells},
	url = {https://products.aspose.com/cells/},
	author = {{Aspose}},
}

@misc{chen_evaluating_2021,
	title = {Evaluating Large Language Models Trained on Code},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a {GPT} language model fine-tuned on publicly available code from {GitHub}, and study its Python code-writing capabilities. A distinct production version of Codex powers {GitHub} Copilot. On {HumanEval}, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while {GPT}-3 solves 0\% and {GPT}-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	number = {{arXiv}:2107.03374},
	publisher = {{arXiv}},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and {McGrew}, Bob and Amodei, Dario and {McCandlish}, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	urldate = {2025-01-31},
	date = {2021-07-14},
	eprinttype = {arxiv},
	eprint = {2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{amalfitano_exact_2016,
	title = {{EXACT}: A tool for comprehending {VBA}-based Excel spreadsheet applications},
	volume = {28},
	rights = {Copyright © 2016 John Wiley \& Sons, Ltd.},
	issn = {2047-7481},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.1787},
	doi = {10.1002/smr.1787},
	shorttitle = {{EXACT}},
	abstract = {Spreadsheet applications are widely adopted by millions of end users from several application domains and provide strategic support to many business, scientific, industrial, and organizational processes. These applications are usually developed by rapid application development processes, exploiting host scripting languages allowing the basic spreadsheets to provide complex functionality, business rules, and user interfaces. Several factors complicate the comprehension of these applications because they are usually developed and maintained by end users without specific software engineering skills, grow over time, are not adequately documented, and do not present explicit separation between data, business logic, and user interface layers. This paper presents a reverse engineering tool intended to support the comprehension of Excel spreadsheet applications developed using the Visual Basic for Application programming language. The tool has been implemented as an add-in that extends the Excel working environment by providing analysis and visualization features. It is able to extract information about the elements composing the analyzed Excel spreadsheet application, the functionality it exposes through its user interface, and the dependencies among its cells. This information is provided by means of interactive views. The validity of the tool has been assessed by a qualitative case study performed with professional end users from an automotive industrial domain. Copyright © 2016 John Wiley \& Sons, Ltd.},
	pages = {483--505},
	number = {6},
	journaltitle = {Journal of Software: Evolution and Process},
	author = {Amalfitano, Domenico and De Simone, Vincenzo and Fasolino, Anna Rita and Tramontana, Porfirio},
	urldate = {2025-01-13},
	date = {2016},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.1787},
	keywords = {Excel reverse engineering, analysis of visual basic for applications, industrial case study, spreadsheet comprehension, visualization tool},
}

@article{yang_exploring_2024,
	title = {Exploring and Unleashing the Power of Large Language Models in Automated Code Translation},
	volume = {1},
	issn = {2994-970X},
	url = {https://dl.acm.org/doi/10.1145/3660778},
	doi = {10.1145/3660778},
	abstract = {Code translation tools, namely transpilers, are developed for automatic source-to-source translation. Latest learning-based transpilers have shown impressive enhancement against rule-based counterparts in both translation accuracy and readability, owing to their task-specific pre-training on extensive monolingual corpora. Nevertheless, their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive. Large Language Models ({LLMs}), pre-trained on huge amounts of human-written code/text, have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific re-training/fine-tuning. Thus, {LLMs} can potentially circumvent the above limitations, but they have not been exhaustively explored yet. This paper investigates diverse {LLMs} and learning-based transpilers for automated code translation tasks, finding that: although certain {LLMs} have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs (38.51\%), missing clear instructions on I/O types in translation (14.94\%), and ignoring discrepancies between source and target programs (41.38\%).  Enlightened by the above findings, we further propose
              {UniTrans}
              , a
              Uni
              fied code
              Trans
              lation framework, applicable to various {LLMs}, for unleashing their power in this field. Specifically,
              {UniTrans}
              first crafts a series of test cases for target programs with the assistance of source programs. Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution. Afterward,
              {UniTrans}
              further (iteratively) repairs incorrectly translated programs prompted by test case execution results. Extensive experiments are conducted on six settings of translation datasets between Python, Java, and C++. Three recent {LLMs} of diverse sizes, including {GPT}-3.5 and {LLaMA}-13B/7B, are tested with
              {UniTrans}
              , and all achieve substantial improvements in terms of computational accuracy and exact match accuracy among almost all translation settings, showing the universal effectiveness of
              {UniTrans}
              in practice.},
	pages = {1585--1608},
	issue = {{FSE}},
	journaltitle = {Proceedings of the {ACM} on Software Engineering},
	shortjournal = {Proc. {ACM} Softw. Eng.},
	author = {Yang, Zhen and Liu, Fang and Yu, Zhongxing and Keung, Jacky Wai and Li, Jia and Liu, Shuo and Hong, Yifan and Ma, Xiaoxue and Jin, Zhi and Li, Ge},
	urldate = {2025-01-29},
	date = {2024-07-12},
	langid = {english},
}

@article{yang_finding_nodate,
	title = {Finding and Understanding Bugs in C Compilers},
	abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to ﬁnd compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our ﬁrst contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undeﬁned and unspeciﬁed behaviors that would destroy its ability to automatically ﬁnd wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
	author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
	langid = {english},
}

@article{combettes_fixed_2021,
	title = {Fixed Point Strategies in Data Science},
	volume = {69},
	issn = {1941-0476},
	url = {https://ieeexplore.ieee.org/document/9390397},
	doi = {10.1109/TSP.2021.3069677},
	abstract = {The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.},
	pages = {3878--3905},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	author = {Combettes, Patrick L. and Pesquet, Jean-Christophe},
	urldate = {2025-01-30},
	date = {2021},
	note = {Conference Name: {IEEE} Transactions on Signal Processing},
	keywords = {Convex functions, Convex optimization, Data science, Inverse problems, Neural networks, Signal processing algorithms, Standards, Tools, fixed point, game theory, image recovery, inverse problems, machine learning, monotone inclusion, neural networks, nonexpansive operator, signal processing},
}

@article{ishikawa_fixed_1974,
	title = {Fixed points by a new iteration method},
	volume = {44},
	issn = {0002-9939, 1088-6826},
	url = {https://www.ams.org/proc/1974-044-01/S0002-9939-1974-0336469-5/},
	doi = {10.1090/S0002-9939-1974-0336469-5},
	abstract = {The following result is shown. If
              
                
                  
                    T
                    T
                  
                
              
              is a lipschitzian pseudo-contractive map of a compact convex subset
              
                
                  
                    E
                    E
                  
                
              
              of a Hilbert space into itself and
              
                
                  
                    
                      
                        x
                        1
                      
                    
                    \{x\_1\}
                  
                
              
              is any point in
              
                
                  
                    E
                    E
                  
                
              
              , then a certain mean value sequence defined by
              
                
                  
                    
                      
                        
                          x
                          
                            n
                            +
                            1
                          
                        
                      
                      =
                      
                        
                          
                            α
                            
                          
                          n
                        
                      
                      T
                      [
                      
                        
                          
                            β
                            
                          
                          n
                        
                      
                      T
                      
                        
                          x
                          n
                        
                      
                      +
                      (
                      1
                      
                        −
                        
                      
                      
                        
                          
                            β
                            
                          
                          n
                        
                      
                      )
                      
                        
                          x
                          n
                        
                      
                      ]
                      +
                      (
                      1
                      
                        −
                        
                      
                      
                        
                          
                            α
                            
                          
                          n
                        
                      
                      )
                      
                        
                          x
                          n
                        
                      
                    
                    \{x\_\{n + 1\}\} = \{{\textbackslash}alpha \_n\}T[\{{\textbackslash}beta \_n\}T\{x\_n\} + (1 - \{{\textbackslash}beta \_n\})\{x\_n\}] + (1 - \{{\textbackslash}alpha \_n\})\{x\_n\}
                  
                
              
              converges strongly to a fixed point of
              
                
                  
                    T
                    T
                  
                
              
              , where
              
                
                  
                    
                      \{
                      
                        
                          
                            α
                            
                          
                          n
                        
                      
                      \}
                    
                    {\textbackslash}\{ \{{\textbackslash}alpha \_n\}{\textbackslash}\}
                  
                
              
              and
              
                
                  
                    
                      \{
                      
                        
                          
                            β
                            
                          
                          n
                        
                      
                      \}
                    
                    {\textbackslash}\{ \{{\textbackslash}beta \_n\}{\textbackslash}\}
                  
                
              
              are sequences of positive numbers that satisfy some conditions.},
	pages = {147--150},
	number = {1},
	journaltitle = {Proceedings of the American Mathematical Society},
	shortjournal = {Proc. Amer. Math. Soc.},
	author = {Ishikawa, Shiro},
	urldate = {2025-02-05},
	date = {1974-05},
	langid = {english},
}

@article{cambronero_flashfill_2023,
	title = {{FlashFill}++: Scaling Programming by Example by Cutting to the Chase},
	volume = {7},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3571226},
	doi = {10.1145/3571226},
	shorttitle = {{FlashFill}++},
	abstract = {Programming-by-Examples ({PBE}) involves synthesizing an "intended program" from a small set of user-provided input-output examples. A key {PBE} strategy has been to restrict the search to a carefully designed small domain-specific language ({DSL}) with "effectively-invertible" ({EI}) operators at the top and "effectively-enumerable" ({EE}) operators at the bottom. This facilitates an effective combination of top-down synthesis strategy (which backpropagates outputs over various paths in the {DSL} using inverse functions) with a bottom-up synthesis strategy (which propagates inputs over various paths in the {DSL}). We address the problem of scaling synthesis to large {DSLs} with several non-{EI}/{EE} operators. This is motivated by the need to support a richer class of transformations and the need for readable code generation. We propose a novel solution strategy that relies on propagating fewer values and over fewer paths.
            Our first key idea is that of "cut functions" that prune the set of values being propagated by using knowledge of the sub-{DSL} on the other side. Cuts can be designed to preserve completeness of synthesis; however, {DSL} designers may use incomplete cuts to have finer control over the kind of programs synthesized. In either case, cuts make search feasible for non-{EI}/{EE} operators and efficient for deep {DSLs}. Our second key idea is that of "guarded {DSLs}" that allow a precedence on {DSL} operators, which dynamically controls exploration of various paths in the {DSL}. This makes search efficient over grammars with large fanouts without losing recall. It also makes ranking simpler yet more effective in learning an intended program from very few examples. Both cuts and precedence provide a mechanism to the {DSL} designer to restrict search to a reasonable, and possibly incomplete, space of programs.
            Using cuts and {gDSLs}, we have built {FlashFill}++, an industrial-strength {PBE} engine for performing rich string transformations, including datetime and number manipulations. The {FlashFill}++ {gDSL} is designed to enable readable code generation in different target languages including Excel's formula language, {PowerFx}, and Python. We show {FlashFill}++ is more expressive, more performant, and generates better quality code than comparable existing {PBE} systems. {FlashFill}++ is being deployed in several mass-market products ranging from spreadsheet software to notebooks and business intelligence applications, each with millions of users.},
	pages = {952--981},
	issue = {{POPL}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Cambronero, José and Gulwani, Sumit and Le, Vu and Perelman, Daniel and Radhakrishna, Arjun and Simon, Clint and Tiwari, Ashish},
	urldate = {2025-01-15},
	date = {2023-01-09},
	langid = {english},
}

@online{microsoft_floating-point_nodate,
	title = {Floating-point arithmetic may give inaccurate result in Excel - Microsoft 365 Apps},
	url = {https://learn.microsoft.com/en-us/troubleshoot/microsoft-365-apps/excel/floating-point-arithmetic-inaccurate-result},
	abstract = {Discusses that floating-point arithmetic may give inaccurate results in Excel.},
	author = {Microsoft},
	urldate = {2025-09-11},
	langid = {english},
}

@article{sujeeth_forge_2014,
	title = {Forge: generating a high performance {DSL} implementation from a declarative specification},
	volume = {49},
	issn = {0362-1340, 1558-1160},
	url = {https://dl.acm.org/doi/10.1145/2637365.2517220},
	doi = {10.1145/2637365.2517220},
	shorttitle = {Forge},
	abstract = {Domain-specific languages provide a promising path to automatically compile high-level code to parallel, heterogeneous, and distributed hardware. However, in practice high performance {DSLs} still require considerable software expertise to develop and force users into tool-chains that hinder prototyping and debugging. To address these problems, we present Forge, a new meta {DSL} for declaratively specifying high performance embedded {DSLs}. Forge provides {DSL} authors with high-level abstractions (e.g., data structures, parallel patterns, effects) for specifying their {DSL} in a way that permits high performance. From this high-level specification, Forge automatically generates both a naïve Scala library implementation of the {DSL} and a high performance version using the Delite {DSL} framework. Users of a Forge-generated {DSL} can prototype their application using the library version, and then switch to the Delite version to run on multicore {CPUs}, {GPUs}, and clusters without changing the application code. Forge-generated Delite {DSLs} perform within 2x of hand-optimized C++ and up to 40x better than Spark, an alternative high-level distributed programming environment. Compared to a manually implemented Delite {DSL}, Forge provides a factor of 3-6x reduction in lines of code and does not sacrifice any performance. Furthermore, Forge specifications can be generated from existing Scala libraries, are easy to maintain, shield {DSL} developers from changes in the Delite framework, and enable {DSLs} to be retargeted to other frameworks transparently.},
	pages = {145--154},
	number = {3},
	journaltitle = {{ACM} {SIGPLAN} Notices},
	shortjournal = {{SIGPLAN} Not.},
	author = {Sujeeth, Arvind K. and Gibbons, Austin and Brown, Kevin J. and Lee, {HyoukJoong} and Rompf, Tiark and Odersky, Martin and Olukotun, Kunle},
	urldate = {2025-01-27},
	date = {2014-03-05},
	langid = {english},
}

@inproceedings{garrido_formal_2006,
	location = {Philadelphia, {PA}, {USA}},
	title = {Formal Specification and Verification of Java Refactorings},
	isbn = {978-0-7695-2353-8},
	url = {http://ieeexplore.ieee.org/document/4026866/},
	doi = {10.1109/SCAM.2006.16},
	abstract = {There is an extensive literature about refactorings of object-oriented programs, and many refactoring tools for the Java programming language. However, except for a few studies, in practice it is difﬁcult to ﬁnd precise formal speciﬁcations of the preconditions and mechanisms of automated refactorings. Moreover, there is usually no formal proof that a refactoring is correct, i.e., that it preserves the behavior of the program.},
	eventtitle = {2006 Sixth {IEEE} International Workshop on Source Code Analysis and Manipulation},
	pages = {165--174},
	booktitle = {2006 Sixth {IEEE} International Workshop on Source Code Analysis and Manipulation},
	publisher = {{IEEE}},
	author = {Garrido, Alejandra and Meseguer, Jose},
	urldate = {2025-01-27},
	date = {2006-09},
	langid = {english},
}

@inproceedings{cunha_relational_2012,
	location = {Trento Italy},
	title = {From relational {ClassSheets} to {UML}+{OCL}},
	isbn = {978-1-4503-0857-1},
	url = {https://dl.acm.org/doi/10.1145/2245276.2231957},
	doi = {10.1145/2245276.2231957},
	eventtitle = {{SAC} 2012: {ACM} Symposium on Applied Computing},
	pages = {1151--1158},
	booktitle = {Proceedings of the 27th Annual {ACM} Symposium on Applied Computing},
	publisher = {{ACM}},
	author = {Cunha, Jácome and Fernandes, João Paulo and Saraiva, João},
	urldate = {2025-01-13},
	date = {2012-03-26},
	langid = {english},
}

@inproceedings{cunha_spreadsheets_2009,
	location = {New York, {NY}, {USA}},
	title = {From spreadsheets to relational databases and back},
	isbn = {978-1-60558-327-3},
	url = {https://doi.org/10.1145/1480945.1480972},
	doi = {10.1145/1480945.1480972},
	series = {{PEPM} '09},
	abstract = {This paper presents techniques and tools to transform spreadsheets into relational databases and back. A set of data refinement rules is introduced to map a tabular datatype into a relational database schema. Having expressed the transformation of the two data models as data refinements, we obtain for free the functions that migrate the data. We use well-known relational database techniques to optimize and query the data. Because data refinements define bi-directional transformations we can map such database back to an optimized spreadsheet. We have implemented the data refinement rules and we constructed Haskell-based tools to manipulate, optimize and refactor Excel-like spreadsheets.},
	pages = {179--188},
	booktitle = {Proceedings of the 2009 {ACM} {SIGPLAN} workshop on Partial evaluation and program manipulation},
	publisher = {Association for Computing Machinery},
	author = {Cunha, Jácome and Saraiva, João and Visser, Joost},
	urldate = {2025-01-13},
	date = {2009-01-19},
}

@thesis{rask_funsheet_2014,
	title = {Funsheet: Integration of Sheet-Defined Functions in Excel using C\#},
	url = {https://www.itu.dk/people/sestoft/funcalc/RaskTimmermann2014.pdf},
	institution = {{IT} University of Copenhagen},
	type = {Master Thesis},
	author = {Rask, Jonas Druedahl and Timmermann, Simon Eikeland},
	urldate = {2025-01-26},
	date = {2014},
}

@inproceedings{yallop_generating_2019,
	location = {Cascais Portugal},
	title = {Generating mutually recursive definitions},
	isbn = {978-1-4503-6226-9},
	url = {https://dl.acm.org/doi/10.1145/3294032.3294078},
	doi = {10.1145/3294032.3294078},
	abstract = {Many functional programs — state machines (Krishnamurthi 2006), top-down and bottom-up parsers (Hutton and Meijer 1996; Hinze and Paterson 2003), evaluators (Abelson et al. 1984), {GUI} initialization graphs (Syme 2006), \&c. — are conveniently expressed as groups of mutually recursive bindings. One therefore expects program generators, such as those written in {MetaOCaml}, to be able to build programs with mutual recursion.},
	eventtitle = {{POPL} '19: 46th Annual {ACM} {SIGPLAN} Symposium on Principles of Programming Languages},
	pages = {75--81},
	booktitle = {Proceedings of the 2019 {ACM} {SIGPLAN} Workshop on Partial Evaluation and Program Manipulation},
	publisher = {{ACM}},
	author = {Yallop, Jeremy and Kiselyov, Oleg},
	urldate = {2025-01-30},
	date = {2019-01-14},
	langid = {english},
}

@inproceedings{guo_graphcodebert_2020,
	title = {{GraphCodeBERT}: Pre-training Code Representations with Data Flow},
	url = {https://openreview.net/forum?id=jLoC4ez43PZ},
	shorttitle = {{GraphCodeBERT}},
	abstract = {Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present {GraphCodeBERT}, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree ({AST}), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of "where-the-value-comes-from" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of {AST}, the property of which makes the model more efficient. We develop {GraphCodeBERT} based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve {GraphCodeBERT} and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.},
	eventtitle = {International Conference on Learning Representations},
	author = {Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and Tufano, Michele and Deng, Shao Kun and Clement, Colin and Drain, Dawn and Sundaresan, Neel and Yin, Jian and Jiang, Daxin and Zhou, Ming},
	urldate = {2025-01-29},
	date = {2020-10-02},
	langid = {english},
}

@online{noauthor_httpswwwcssjtueducnkzhucs383pierce_types_programming_languagespdf_nodate,
	title = {https://www.cs.sjtu.edu.cn/{\textasciitilde}kzhu/cs383/Pierce\_Types\_Programming\_Languages.pdf},
	url = {https://www.cs.sjtu.edu.cn/~kzhu/cs383/Pierce_Types_Programming_Languages.pdf},
	urldate = {2025-05-13},
}

@inproceedings{borstler_i_2018,
	location = {Bologna Italy},
	title = {"I know it when I see it" Perceptions of Code Quality: {ITiCSE} '17 Working Group Report},
	isbn = {978-1-4503-5627-5},
	url = {https://dl.acm.org/doi/10.1145/3174781.3174785},
	doi = {10.1145/3174781.3174785},
	shorttitle = {"I know it when I see it" Perceptions of Code Quality},
	abstract = {Context. Code quality is a key issue in software development. The ability to develop high quality software is therefore a key learning goal of computing programs. However, there are no universally accepted measures to assess the quality of code and current standards are considered weak. Furthermore, there are many facets to code quality. Defining and explaining the concept of code quality is therefore a challenge faced by many educators.},
	eventtitle = {{ITiCSE} '17: Innovation and Technology in Computer Science Education},
	pages = {70--85},
	booktitle = {Proceedings of the 2017 {ITiCSE} Conference on Working Group Reports},
	publisher = {{ACM}},
	author = {Börstler, Jürgen and Störrle, Harald and Toll, Daniel and Van Assema, Jelle and Duran, Rodrigo and Hooshangi, Sara and Jeuring, Johan and Keuning, Hieke and Kleiner, Carsten and {MacKellar}, Bonnie},
	urldate = {2025-09-04},
	date = {2018-01-30},
	langid = {english},
}

@inreference{lea_idiomatic_2025,
	location = {Oxford},
	title = {Idiomatic},
	url = {https://www.oxfordlearnersdictionaries.com/definition/english/idiomatic},
	booktitle = {Oxford Learners Dictionary},
	publisher = {Oxford University Press},
	author = {Lea, Diana and Bradbery, Jennifer},
	date = {2025},
}

@misc{noauthor_ieee_2019,
	title = {{IEEE} Standard for Floating-Point Arithmetic},
	number = {{IEEE} 754-2019},
	publisher = {{IEEE}},
	date = {2019-07-22},
}

@misc{johnson_intermediate_2010,
	title = {Intermediate Representation},
	url = {https://web.stanford.edu/class/archive/cs/cs143/cs143.1128/handouts/230%20Intermediate%20Rep.pdf},
	publisher = {Stanford},
	author = {Johnson, Maggie and Zelenski, Julie},
	urldate = {2025-06-24},
	date = {2010-07},
}

@book{kelley_iterative_1995,
	title = {Iterative Methods for Linear and Nonlinear Equations},
	isbn = {978-0-89871-352-7 978-1-61197-094-4},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9781611970944},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kelley, C. T.},
	urldate = {2025-02-05},
	date = {1995-01},
	langid = {english},
	doi = {10.1137/1.9781611970944},
}

@online{gordon_lambda_2021,
	title = {{LAMBDA}: The ultimate Excel worksheet},
	url = {https://www.microsoft.com/en-us/research/blog/lambda-the-ultimatae-excel-worksheet-function/?OCID=msr_blog_lambda_tw},
	titleaddon = {Microsoft Research Blog},
	author = {Gordon, Andy and Peyton Jones, Simon},
	date = {2021-01-25},
}

@online{microsoft_managed_2025,
	title = {Managed Execution Process - .{NET}},
	url = {https://learn.microsoft.com/en-us/dotnet/standard/managed-execution-process},
	abstract = {Learn more about: Managed Execution Process},
	author = {{Microsoft}},
	urldate = {2025-09-13},
	date = {2025},
	langid = {english},
}

@article{mann_mean_1953,
	title = {Mean value methods in iteration},
	volume = {4},
	issn = {0002-9939, 1088-6826},
	url = {https://www.ams.org/proc/1953-004-03/S0002-9939-1953-0054846-3/},
	doi = {10.1090/S0002-9939-1953-0054846-3},
	pages = {506--510},
	number = {3},
	journaltitle = {Proceedings of the American Mathematical Society},
	shortjournal = {Proc. Amer. Math. Soc.},
	author = {Mann, W. Robert},
	urldate = {2025-02-05},
	date = {1953-06},
	langid = {english},
}

@inproceedings{jaffe_meaningful_2018,
	location = {New York, {NY}, {USA}},
	title = {Meaningful variable names for decompiled code: a machine translation approach},
	isbn = {978-1-4503-5714-2},
	url = {https://dl.acm.org/doi/10.1145/3196321.3196330},
	doi = {10.1145/3196321.3196330},
	series = {{ICPC} '18},
	shorttitle = {Meaningful variable names for decompiled code},
	abstract = {When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 {TB} of C source code obtained from {GitHub}. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2\% of the time, which represents a lower bound for performance.},
	pages = {20--30},
	booktitle = {Proceedings of the 26th Conference on Program Comprehension},
	publisher = {Association for Computing Machinery},
	author = {Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Le Goues, Claire and Vasilescu, Bogdan},
	urldate = {2025-01-25},
	date = {2018-05-28},
}

@article{fakhoury_measuring_2020,
	title = {Measuring the impact of lexical and structural inconsistencies on developers’ cognitive load during bug localization},
	volume = {25},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-019-09751-4},
	doi = {10.1007/s10664-019-09751-4},
	abstract = {A large portion of the cost of any software lies in the time spent by developers in understanding a program’s source code before any changes can be undertaken. Measuring program comprehension is not a trivial task. In fact, different studies use self-reported and various psycho-physiological measures as proxies. In this research, we propose a methodology using functional Near Infrared Spectroscopy ({fNIRS}) and eye tracking devices as an objective measure of program comprehension that allows researchers to conduct studies in environments close to real world settings, at identifier level of granularity. We validate our methodology and apply it to study the impact of lexical, structural, and readability issues on developers’ cognitive load during bug localization tasks. Our study involves 25 undergraduate and graduate students and 21 metrics. Results show that the existence of lexical inconsistencies in the source code significantly increases the cognitive load experienced by participants not only on identifiers involved in the inconsistencies but also throughout the entire code snippet. We did not find statistical evidence that structural inconsistencies increase the average cognitive load that participants experience, however, both types of inconsistencies result in lower performance in terms of time and success rate. Finally, we observe that self-reported task difficulty, cognitive load, and fixation duration do not correlate and appear to be measuring different aspects of task difficulty.},
	pages = {2140--2178},
	number = {3},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Fakhoury, Sarah and Roy, Devjeet and Ma, Yuzhan and Arnaoudova, Venera and Adesope, Olusola},
	urldate = {2025-09-08},
	date = {2020-05-01},
	langid = {english},
	keywords = {Biometrics, Cognitive load, Linguistic Antipatterns, Program comprehension, Readability, {fNIRS}},
}

@article{cunha_model_2016,
	title = {Model inference for spreadsheets},
	volume = {23},
	issn = {1573-7535},
	url = {https://doi.org/10.1007/s10515-014-0167-x},
	doi = {10.1007/s10515-014-0167-x},
	abstract = {Many errors in spreadsheet formulas can be avoided if spreadsheets are built automatically from higher-level models that can encode and enforce consistency constraints in the generated spreadsheets. Employing this strategy for legacy spreadsheets is difficult, because the model has to be reverse engineered from an existing spreadsheet and existing data must be transferred into the new model-generated spreadsheet. We have developed and implemented a technique that automatically infers relational schemas from spreadsheets. This technique uses particularities from the spreadsheet realm to create better schemas. We have evaluated this technique in two ways: first, we have demonstrated its applicability by using it on a set of real-world spreadsheets. Second, we have run an empirical study with users. The study has shown that the results produced by our technique are comparable to the ones developed by experts starting from the same (legacy) spreadsheet data. Although relational schemas are very useful to model data, they do not fit spreadsheets well, as they do not allow expressing layout. Thus, we have also introduced a mapping between relational schemas and {ClassSheets}. A {ClassSheet} controls further changes to the spreadsheet and safeguards it against a large class of formula errors. The developed tool is a contribution to spreadsheet (reverse) engineering, because it fills an important gap and allows a promising design method ({ClassSheets}) to be applied to a huge collection of legacy spreadsheets with minimal effort.},
	pages = {361--392},
	number = {3},
	journaltitle = {Automated Software Engineering},
	shortjournal = {Autom Softw Eng},
	author = {Cunha, Jácome and Erwig, Martin and Mendes, Jorge and Saraiva, João},
	urldate = {2025-01-13},
	date = {2016-09-01},
	langid = {english},
	keywords = {Artificial Intelligence, Automatic model inference, {ClassSheets}, Empirical validation, Relational model, Spreadsheets},
}

@inproceedings{steinhofel_modular_2018,
	location = {Cham},
	title = {Modular, Correct Compilation with Automatic Soundness Proofs},
	isbn = {978-3-030-03418-4},
	doi = {10.1007/978-3-030-03418-4_25},
	abstract = {Formal verification of compiler correctness requires substantial effort. A particular challenge is lack of modularity and automation. Any change or update to the compiler can render existing proofs obsolete and cause considerable manual proof effort. We propose a framework for automatically proving the correctness of compilation rules based on simultaneous symbolic execution for the source and target language. The correctness of the whole system follows from the correctness of each compilation rule. To support a new source or target language it is sufficient to formalize that language in terms of symbolic execution, while the corresponding formalization of its counterpart can be re-used. The correctness of translation rules can be checked automatically. Our approach is based on a reduction of correctness assertions to formulas in a program logic capable of symbolic execution of abstract programs. We instantiate the framework for compilation from Java to {LLVM} {IR} and provide a symbolic execution system for a subset of {LLVM} {IR}.},
	pages = {424--447},
	booktitle = {Leveraging Applications of Formal Methods, Verification and Validation. Modeling},
	publisher = {Springer International Publishing},
	author = {Steinhöfel, Dominic and Hähnle, Reiner},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	date = {2018},
	langid = {english},
}

@inproceedings{kahn_natural_1987,
	location = {Berlin, Heidelberg},
	title = {Natural semantics},
	isbn = {978-3-540-47419-7},
	doi = {10.1007/BFb0039592},
	abstract = {During the past few years, many researchers have begun to present semantic specifications in a style that has been strongly advocated by Plotkin in [19]. The purpose of this paper is to introduce in an intuitive manner the essential ideas of the method that we call now Natural Semantics, together with its connections to ideas in logic and computing. Natural Semantics is of interest per se and because it is used as a semantics specification formalism for an interactive computer system that we are currently building at {INRIA}.},
	pages = {22--39},
	booktitle = {{STACS} 87},
	publisher = {Springer},
	author = {Kahn, G.},
	editor = {Brandenburg, Franz J. and Vidal-Naquet, Guy and Wirsing, Martin},
	date = {1987},
	langid = {english},
	keywords = {Abstract Syntax, Inference Rule, Natural Deduction, Static Semantic, Syntactic Category},
}

@article{noor_new_2000,
	title = {New Approximation Schemes for General Variational Inequalities},
	volume = {251},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0022247X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022247X00970422},
	doi = {10.1006/jmaa.2000.7042},
	pages = {217--229},
	number = {1},
	journaltitle = {Journal of Mathematical Analysis and Applications},
	shortjournal = {Journal of Mathematical Analysis and Applications},
	author = {Noor, Muhammad Aslam},
	urldate = {2025-02-05},
	date = {2000-11},
	langid = {english},
}

@article{thakur_new_2014,
	title = {New iteration scheme for numerical reckoning fixed points of nonexpansive mappings},
	volume = {2014},
	issn = {1029-242X},
	url = {https://doi.org/10.1186/1029-242X-2014-328},
	doi = {10.1186/1029-242X-2014-328},
	abstract = {The purpose of this paper is to introduce a new three step iteration scheme for approximation of fixed points of the nonexpansive mappings. We show that our iteration process is faster than all of the Picard, the Mann, the Agarwal et al., and the Abbas et al. iteration processes. We support our analytic proof by a numerical example in which we approximate the fixed point by a computer using Matlab program. We also prove some weak convergence and strong convergence theorems for the nonexpansive mappings.},
	pages = {328},
	number = {1},
	journaltitle = {Journal of Inequalities and Applications},
	shortjournal = {Journal of Inequalities and Applications},
	author = {Thakur, Dipti and Thakur, Balwant Singh and Postolache, Mihai},
	urldate = {2025-02-05},
	date = {2014-09-02},
	keywords = {fixed point, nonexpansive mapping, strong and weak convergence theorems},
}

@article{kaser_conversion_1993,
	title = {On the conversion of indirect to direct recursion},
	volume = {2},
	issn = {1057-4514},
	url = {https://dl.acm.org/doi/10.1145/176454.176510},
	doi = {10.1145/176454.176510},
	abstract = {Procedure inlining can be used to convert mutual recursion to direct recursion. This allows use of optimization techniques that are most easily applied to directly recursive procedures, in addition to the well-known benefits of inlining. We present tight (necessary and sufficient) conditions under which inlining can transform all mutual recursion to direct recursion, and those under which heuristics to eliminate mutual recursion always terminate. We also present a technique to eliminate mutually recursive circuits that consist of only tail calls. From this, we conclude that tail recursion elimination should be interleaved with inlining.},
	pages = {151--164},
	number = {1},
	journaltitle = {{ACM} Lett. Program. Lang. Syst.},
	author = {Kaser, Owen and Ramakrishnan, C. R. and Pawagi, Shaunak},
	urldate = {2025-07-20},
	date = {1993-03-01},
}

@article{phuengrattana_rate_2011,
	title = {On the rate of convergence of Mann, Ishikawa, Noor and {SP}-iterations for continuous functions on an arbitrary interval},
	volume = {235},
	issn = {0377-0427},
	url = {https://www.sciencedirect.com/science/article/pii/S0377042710006722},
	doi = {10.1016/j.cam.2010.12.022},
	abstract = {In this paper, we propose a new iteration, called the {SP}-iteration, for approximating a fixed point of continuous functions on an arbitrary interval. Then, a necessary and sufficient condition for the convergence of the {SP}-iteration of continuous functions on an arbitrary interval is given. We also compare the convergence speed of Mann, Ishikawa, Noor and {SP}-iterations. It is proved that the {SP}-iteration is equivalent to and converges faster than the others. Our results extend and improve the corresponding results of Borwein and Borwein [D. Borwein, J. Borwein, Fixed point iterations for real functions, J. Math. Anal. Appl. 157 (1991) 112–126], Qing and Qihou [Y. Qing, L. Qihou, The necessary and sufficient condition for the convergence of Ishikawa iteration on an arbitrary interval, J. Math. Anal. Appl. 323 (2006) 1383–1386], Rhoades [B.E. Rhoades, Comments on two fixed point iteration methods, J. Math. Anal. Appl. 56 (1976) 741–750], and many others. Moreover, we also present numerical examples for the {SP}-iteration to compare with the Mann, Ishikawa and Noor iterations.},
	pages = {3006--3014},
	number = {9},
	journaltitle = {Journal of Computational and Applied Mathematics},
	shortjournal = {Journal of Computational and Applied Mathematics},
	author = {Phuengrattana, Withun and Suantai, Suthep},
	urldate = {2025-02-05},
	date = {2011-03-01},
	keywords = {Continuous functions, Convergence theorem, Fixed point, Nondecreasing functions, Rate of convergence},
}

@article{bock_semantics_2020,
	title = {On the semantics for spreadsheets with sheet-defined functions},
	volume = {57},
	issn = {2590-1184},
	url = {https://www.sciencedirect.com/science/article/pii/S2590118420300204},
	doi = {10.1016/j.cola.2020.100960},
	abstract = {We give an operational semantics for the evaluation of spreadsheets, including sheet-defined and built-in numeric functions in the Funcalc spreadsheet platform. The semantics allows for different implementations and we discuss sheet-defined functions implemented using both interpretation and run-time code generation. The semantics specifies the expected result of a computation, also considering non-deterministic functions, independently of an evaluation mechanism. It can be extended to include the cost of formula evaluation for a cost analysis e.g. for use in parallelization of computations. An interesting future direction is to investigate experimentally how close our semantics is to that of major spreadsheet implementations.},
	pages = {100960},
	journaltitle = {Journal of Computer Languages},
	shortjournal = {Journal of Computer Languages},
	author = {Bock, Alexander Asp and Bøgholm, Thomas and Sestoft, Peter and Thomsen, Bent and Thomsen, Lone Leth},
	urldate = {2025-01-26},
	date = {2020-04-01},
	keywords = {Funcalc, Recalculation, Semantics, Sheet-defined function, Spreadsheet},
}

@thesis{poulsen_optimized_2007,
	title = {Optimized Recalculation for Spreadsheets with the Use of Support Graph},
	institution = {{IT} University of Copenhagen},
	type = {Master Thesis},
	author = {Poulsen, Morten and Serek, Poul Peter},
	date = {2007},
	langid = {english},
}

@article{bock_parallel_2021,
	title = {Parallel spreadsheet evaluation and dynamic cycle detection},
	volume = {33},
	issn = {1532-0626, 1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpe.6218},
	doi = {10.1002/cpe.6218},
	abstract = {It was estimated there would be 72 million users using spreadsheets monthly in 2017 some of which build complex ﬁnancial, scientiﬁc and mathematical models. Most of these end-users are not trained {IT} professionals but domain experts. In the age of multicore computing and ever-increasing amounts of data, how can end-users access this powerful, parallel hardware to accelerate spreadsheet computation? Some existing solutions are usually not fully automatic and require a level of interaction from end-users to facilitate parallel execution. Ideally, an end-user tool would transparently exploit the underlying hardware and automatically discover available parallelism in the spreadsheet without any required interaction.},
	pages = {e6218},
	number = {13},
	journaltitle = {Concurrency and Computation: Practice and Experience},
	shortjournal = {Concurrency and Computation},
	author = {Bock, Alexander Asp},
	urldate = {2025-01-24},
	date = {2021-07-10},
	langid = {english},
}

@inproceedings{hsu_program_2024,
	title = {Program Synthesis on Single-Layer Loop Behavior in Pure Functional Programming},
	url = {https://ieeexplore.ieee.org/document/10612128},
	doi = {10.1109/CEC60901.2024.10612128},
	abstract = {Program synthesis ({PS}) is a field devoted to auto-matically generating computer programs from high-level specifications, and genetic programming ({GP}) is one commonly-used way to achieve {PS}. {PushGP}, operating on a stack-based language, is considered as a state-of-the-art program synthesizer among {GPs}, while another research trend foucus on the grammar-based languages due to the readability and the ease of maintenance. In this paper, we propose the repetitive structure genetic programming ({RSGP}), a new grammar-based program synthesizer under the pure functional programming paradigm. {RSGP} defines a recursive function to simulate the single-layer loop behavior and leverages the minimum redundancy maximum relevance ({\textbackslash}{textmRMR}) feature selection with the Pearson correlation coefficient ({PCC}) to select the capable and diverse programs for the next generation. The experiment results show that {RSGP} outperforms {PushGP}, {CBGP}, and {HOTGP} in terms of the number of successful programs on {CountOdds} and {LastIndexofZero} from {PSBl}, Luhn from {PSB}2, and 3 out of 4 designed problems. Additionally, the ablation study indicates that using {\textbackslash}{textmRMR} with {PCC} does encourage proper problem decomposition with the trade-off of diminishing the search ability within a similar neighborhood. {RSGP} utilizes an adaptation mechanism to balance the trade-off to automatically fit the needs of different problems.},
	eventtitle = {2024 {IEEE} Congress on Evolutionary Computation ({CEC})},
	pages = {1--8},
	booktitle = {2024 {IEEE} Congress on Evolutionary Computation ({CEC})},
	author = {Hsu, Tzu-Hao and Chang, Chi-Hsien and Yu, Tian-Li},
	urldate = {2025-01-13},
	date = {2024-06},
	keywords = {Feature extraction, Genetic Programming, Genetic programming, Maintenance, Market research, Program Synthesis, Redundancy, Search problems, Synthesizers},
}

@article{waters_program_1988,
	title = {Program translation via abstraction and reimplementation},
	volume = {14},
	issn = {1939-3520},
	url = {https://ieeexplore.ieee.org/document/7629/?arnumber=7629},
	doi = {10.1109/32.7629},
	abstract = {An abstraction-and-reimplementation paradigm is presented in which the source program is first analyzed in order to obtain a programming-language-independent abstract understanding of the computation performed by the program as a whole. The program is then reimplemented in the target language based on this understanding. The key to this approach is the abstract understanding obtained. It allows the translator to benefit from an appreciation of the global features of the source program without being distracted by what are considered irrelevant details. Knowledge-based translation via abstraction and reimplementation is described as one of the goals of the Programmer's Apprentice project. A translator which translates Cobol programs into Hibol (a very-high-level business data processing language) has been constructed. A computer which generates extremely efficient {PDP}-11 object code for Pascal programs has been designed.{\textless}{\textgreater}},
	pages = {1207--1228},
	number = {8},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Waters, R.C.},
	urldate = {2025-01-29},
	date = {1988-08},
	note = {Conference Name: {IEEE} Transactions on Software Engineering},
	keywords = {Artificial intelligence, Data processing, Performance analysis, Program processors, Prototypes, Writing},
}

@inproceedings{cockx_reasonable_2022,
	location = {Ljubljana Slovenia},
	title = {Reasonable Agda is correct Haskell: writing verified Haskell using agda2hs},
	isbn = {978-1-4503-9438-3},
	url = {https://dl.acm.org/doi/10.1145/3546189.3549920},
	doi = {10.1145/3546189.3549920},
	shorttitle = {Reasonable Agda is correct Haskell},
	abstract = {Modern dependently typed languages such as Agda can be used to statically enforce the correctness of programs. However, they still lack the large ecosystem of a more popular language like Haskell. To combine the strength of both approaches, we present agda2hs, a tool that translates an expressive subset of Agda to readable Haskell, erasing dependent types and proofs in the process. Thanks to Agda’s support for erasure annotations, this process is both safe and transparent to the user. Compared to other tools for program extraction, agda2hs uses a syntax that is already familiar to functional programmers, allows for both intrinsic and extrinsic approaches to verification, and produces Haskell code that is easy to read and audit by programmers with no knowledge of Agda.},
	eventtitle = {Haskell '22: 15th {ACM} {SIGPLAN} International Haskell Symposium},
	pages = {108--122},
	booktitle = {Proceedings of the 15th {ACM} {SIGPLAN} International Haskell Symposium},
	publisher = {{ACM}},
	author = {Cockx, Jesper and Melkonian, Orestis and Escot, Lucas and Chapman, James and Norell, Ulf},
	urldate = {2025-01-29},
	date = {2022-09-06},
	langid = {english},
}

@book{fowler_refactoring_2019,
	location = {Boston Columbus New York San Francisco Amsterdam Cape Town Dubai London Munich},
	edition = {Second edition},
	title = {Refactoring: improving the design of existing code},
	isbn = {978-0-13-475759-9},
	series = {The Addison-Wesley signature series},
	shorttitle = {Refactoring},
	abstract = {This book offers a thorough discussion of the principles of refactoring, including where to spot opportunities for refactoring, and how to set up the required tests. There is also a catalog of more than 40 proven refactorings with details as to when and why to use the refactoring, step by step instructions for implementing it, and an example illustrating how it works. [Verlagshomepage]},
	pagetotal = {418},
	publisher = {Addison-Wesley},
	author = {Fowler, Martin and Beck, Kent},
	date = {2019},
}

@inproceedings{biermann_rewriting_2018,
	location = {Cham},
	title = {Rewriting High-Level Spreadsheet Structures into Higher-Order Functional Programs},
	isbn = {978-3-319-73305-0},
	doi = {10.1007/978-3-319-73305-0_2},
	abstract = {Spreadsheets are used heavily in industry and academia. Often, spreadsheet models are developed for years and their complexity grows vastly beyond what the paradigm was originally conceived for. Such complexity often comes at the cost of recalculation performance. However, spreadsheet models usually have some high-level structure that can be used to improve performance by performing independent computation in parallel. In this paper, we devise rules for rewriting high-level spreadsheet structure in the form of so-called cell arrays into higher-order functional programs that can be easily parallelized on multicore processors. We implement our rule set for the experimental Funcalc spreadsheet engine which already implements parallelizable higher-order array functions as well as user-defined higher-order functions. Benchmarks show that our rewriting approach improves recalculation performance for spreadsheets that are dominated by cell arrays.},
	pages = {20--35},
	booktitle = {Practical Aspects of Declarative Languages},
	publisher = {Springer International Publishing},
	author = {Biermann, Florian and Dou, Wensheng and Sestoft, Peter},
	editor = {Calimeri, Francesco and Hamlen, Kevin and Leone, Nicola},
	date = {2018},
	langid = {english},
	keywords = {Array Formula, Cell Array, Form C6R, Higher-order Functional Programs, {IN} {TRANSITIONAL} {CELL}},
}

@thesis{iversen_runtime_2006,
	title = {Runtime code generation to speed up spreadsheet computations},
	institution = {{IT} University of Copenhagen},
	type = {Master Thesis},
	author = {Iversen, Thomas S},
	date = {2006-07-30},
	langid = {english},
}

@thesis{iversen_runtime_nodate,
	title = {Runtime code generation to speed up spreadsheet computations},
	type = {phdthesis},
	author = {Iversen, Thomas},
	langid = {english},
}

@article{turk_sdfunc_2022,
	title = {{SDFunc}: Modular spreadsheet design with sheet-defined functions in Microsoft Excel},
	volume = {52},
	rights = {© 2021 John Wiley \& Sons Ltd.},
	issn = {1097-024X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3027},
	doi = {10.1002/spe.3027},
	shorttitle = {{SDFunc}},
	abstract = {The goal of the {SDFunc} tool is to enable spreadsheet developers to build their model computations in Microsoft Excel according to the modular design approach, that is, the separation of the functionalities into independent, interchangeable modules with interfaces that provide input and output elements. This concept has been theoretically developed in recent years and is known as sheet-defined functions in the literature. In this article, we are presenting our implementation of the tool and the evaluation steps that we took to make the tool interesting and suitable for the assessment of the modular approach in spreadsheet development by the industry, specifically within organizational and companies' settings where the spreadsheet developers and end-users involved in experiments expect to use a well-established spreadsheet platform. We also demonstrated that sheet-defined functions can be implemented by development tools already present in Microsoft Excel.},
	pages = {415--426},
	number = {2},
	journaltitle = {Software: Practice and Experience},
	author = {Turk, Tomaž},
	urldate = {2025-01-13},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.3027},
	keywords = {Microsoft Excel, add-in, modular design, sheet-defined function, spreadsheet},
}

@inproceedings{sestoft_sheet-defined_2013,
	location = {Berlin, Heidelberg},
	title = {Sheet-Defined Functions: Implementation and Initial Evaluation},
	isbn = {978-3-642-38706-7},
	doi = {10.1007/978-3-642-38706-7_8},
	shorttitle = {Sheet-Defined Functions},
	abstract = {Spreadsheets are ubiquitous end-user programming tools, but lack even the simplest abstraction mechanism: The ability to encapsulate a computation as a function. This was observed by Peyton-Jones and others [14], who proposed a mechanism to define such functions using only standard spreadsheet cells, formulas and references.},
	pages = {88--103},
	booktitle = {End-User Development},
	publisher = {Springer},
	author = {Sestoft, Peter and Sørensen, Jens Zeilund},
	editor = {Dittrich, Yvonne and Burnett, Margaret and Mørch, Anders and Redmiles, David},
	date = {2013},
	langid = {english},
}

@report{jetbrains_software_2024,
	title = {Software Developers Statistics 2024 - State of Developer Ecosystem Report},
	url = {https://www.jetbrains.com/lp/devecosystem-2024},
	abstract = {Explore key software developer statistics for 2024 in the State of Developer Ecosystem Report. Trends, insights, and tools shaping the developer world},
	number = {2024},
	institution = {{JetBrains}},
	author = {{JetBrains}},
	urldate = {2025-09-03},
	date = {2024},
	langid = {english},
}

@article{rhoades_fixed_1991,
	title = {Some fixed point iteration procedures},
	volume = {14},
	rights = {Copyright © 1991 Hindawi Publishing Corporation.},
	issn = {1687-0425},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/S0161171291000017},
	doi = {10.1155/S0161171291000017},
	abstract = {This paper provides a survey of iteration procedures that have been used to obtain fixed points for maps satisfying a variety of contractive conditions. The author does not claim to provide complete coverage of the literature, and admits to certain biases in the theorems that are cited herein. In spite of these shortcomings, however, this paper should be a useful reference for those persons wishing to become better acquainted with the area.},
	pages = {372065},
	number = {1},
	journaltitle = {International Journal of Mathematics and Mathematical Sciences},
	author = {Rhoades, B. E.},
	urldate = {2025-02-04},
	date = {1991},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/S0161171291000017},
	keywords = {fixed, iteration, point},
}

@article{aslam_noor_iterative_2009,
	title = {Some iterative methods for solving a system of nonlinear equations},
	volume = {57},
	issn = {0898-1221},
	url = {https://www.sciencedirect.com/science/article/pii/S0898122108006196},
	doi = {10.1016/j.camwa.2008.10.067},
	abstract = {In this paper, we suggest and analyze two new two-step iterative methods for solving the system of nonlinear equations using quadrature formulas. We prove that these new methods have cubic convergence. Several numerical examples are given to illustrate the efficiency and the performance of the new iterative methods. These new iterative methods may be viewed as an extension and generalizations of the existing methods for solving the system of nonlinear equations.},
	pages = {101--106},
	number = {1},
	journaltitle = {Computers \& Mathematics with Applications},
	shortjournal = {Computers \& Mathematics with Applications},
	author = {Aslam Noor, Muhammad and Waseem, Muhammad},
	urldate = {2025-02-05},
	date = {2009-01-01},
	keywords = {Efficiency index, Examples, Iterative methods, Nonlinear equations, Quadrature formulas},
}

@article{abdali_spreadsheet_1992,
	title = {Spreadsheet computations in computer algebra},
	volume = {26},
	issn = {0163-5824},
	url = {https://dl.acm.org/doi/10.1145/130933.130936},
	doi = {10.1145/130933.130936},
	abstract = {In this paper, we analyze the concept of spreadsheet computing in computer algebra. Numerical spreadsheet programs have a unique, intuitively clear semantics. But when the variables used in a spreadsheet program are allowed to assume symbolic expressions as values, the meaning of spreadsheet computations is not defined that simply. We consider several alternative models for what spreadsheet computations could mean in the symbolic context. We describe the implementation of spreadsheet features, based on one of these models, in the visual computer algebra environment {MathScribe}.},
	pages = {10--18},
	number = {2},
	journaltitle = {{SIGSAM} Bull.},
	author = {Abdali, S. Kamal and Cherry, Guy W. and Soiffer, Neil},
	urldate = {2025-01-13},
	date = {1992-04-01},
}

@incollection{cunha_spreadsheet_2015,
	location = {Cham},
	title = {Spreadsheet Engineering},
	isbn = {978-3-319-15940-9},
	url = {https://doi.org/10.1007/978-3-319-15940-9_6},
	abstract = {These tutorial notes present a methodology for spreadsheet engineering. First, we present data mining and database techniques to reason about spreadsheet data. These techniques are used to compute relationships between spreadsheet elements (cells/columns/rows), which are later used to infer a model defining the business logic of the spreadsheet. Such a model of a spreadsheet data is a visual domain specific language that we embed in a well-known spreadsheet system.},
	pages = {246--299},
	booktitle = {Central European Functional Programming School: 5th Summer School, {CEFP} 2013, Cluj-Napoca, Romania, July 8-20, 2013, Revised Selected Papers},
	publisher = {Springer International Publishing},
	author = {Cunha, Jácome and Fernandes, João Paulo and Mendes, Jorge and Saraiva, João},
	editor = {Zsók, Viktória and Horváth, Zoltán and Csató, Lehel},
	urldate = {2025-01-13},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-15940-9_6},
	keywords = {Business Logic, Cell Class, Domain Specific Language, Functional Dependency, Unify Modeling Language},
}

@report{sestoft_spreadsheet_2012,
	title = {Spreadsheet technology},
	institution = {{IT}-Universitetet i København},
	type = {Report},
	author = {Sestoft, Peter},
	date = {2012-01-31},
	note = {Publication Title: Spreadsheet technology
Volume: 2011-142},
}

@report{stack_overflow_stack_2024,
	title = {Stack Overflow 2024 Developer Survey},
	url = {https://survey.stackoverflow.co/2024},
	type = {Developer Survey},
	author = {{Stack Overflow}},
	date = {2024},
}

@online{microsoft_stopwatch_2025,
	title = {Stopwatch Class (System.Diagnostics)},
	url = {https://learn.microsoft.com/en-us/dotnet/api/system.diagnostics.stopwatch},
	abstract = {Provides a set of methods and properties that you can use to accurately measure elapsed time.},
	author = {{Microsoft}},
	urldate = {2025-09-08},
	date = {2025},
	langid = {english},
}

@online{microsoft_structure_2025,
	title = {Structure of a {SpreadsheetML} document},
	url = {https://learn.microsoft.com/en-us/office/open-xml/spreadsheet/structure-of-a-spreadsheetml-document},
	abstract = {Use the Open {XML} {SDK} to programmatically create Office Word, Excel, and {PowerPoint} documents, and manipulate their content.},
	author = {{Microsoft}},
	urldate = {2025-09-10},
	date = {2025},
	langid = {english},
}

@software{syncfusion_syncfusion_nodate,
	title = {Syncfusion .{NET} Excel Library},
	url = {https://www.syncfusion.com/document-processing/excel-framework/net/excel-library/formula},
	author = {{Syncfusion}},
}

@incollection{winskel_techniques_1993,
	title = {Techniques for recursion},
	isbn = {978-0-262-29145-3},
	url = {https://direct.mit.edu/books/book/4338/The-Formal-Semantics-of-Programming-LanguagesAn},
	abstract = {The Formal Semantics of Programming Languages provides the basic mathematical techniques necessary for those who are beginning a study of the semantics and logics of programming languages. These techniques will allow students to invent, formalize, and justify rules with which to reason about a variety of programming languages. Although the treatment is elementary, several of the topics covered are drawn from recent research, including the vital area of concurency. The book contains many exercises ranging from simple to miniprojects.Starting with basic set theory, structural operational semantics is introduced as a way to define the meaning of programming languages along with associated proof techniques. Denotational and axiomatic semantics are illustrated on a simple language of while-programs, and fall proofs are given of the equivalence of the operational and denotational semantics and soundness and relative completeness of the axiomatic semantics. A proof of Godel's incompleteness theorem, which emphasizes the impossibility of achieving a fully complete axiomatic semantics, is included. It is supported by an appendix providing an introduction to the theory of computability based on while-programs. Following a presentation of domain theory, the semantics and methods of proof for several functional languages are treated. The simplest language is that of recursion equations with both call-by-value and call-by-name evaluation. This work is extended to lan guages with higher and recursive types, including a treatment of the eager and lazy lambda-calculi. Throughout, the relationship between denotational and operational semantics is stressed, and the proofs of the correspondence between the operation and denotational semantics are provided. The treatment of recursive types - one of the more advanced parts of the book - relies on the use of information systems to represent domains. The book concludes with a chapter on parallel programming languages, accompanied by a discussion of methods for specifying and verifying nondeterministic and parallel programs.},
	pages = {163--181},
	booktitle = {The Formal Semantics of Programming Languages: An Introduction},
	publisher = {The {MIT} Press},
	author = {Winskel, Glynn},
	urldate = {2025-01-31},
	date = {1993-02-05},
	langid = {english},
	doi = {10.7551/mitpress/3054.001.0001},
}

@article{hunter-zinck_ten_2021,
	title = {Ten simple rules on writing clean and reliable open-source scientific software},
	volume = {17},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1009481},
	doi = {10.1371/journal.pcbi.1009481},
	abstract = {Functional, usable, and maintainable open-source software is increasingly essential to scientific research, but there is a large variation in formal training for software development and maintainability. Here, we propose 10 “rules” centered on 2 best practice components: clean code and testing. These 2 areas are relatively straightforward and provide substantial utility relative to the learning investment. Adopting clean code practices helps to standardize and organize software code in order to enhance readability and reduce cognitive load for both the initial developer and subsequent contributors; this allows developers to concentrate on core functionality and reduce errors. Clean coding styles make software code more amenable to testing, including unit tests that work best with modular and consistent software code. Unit tests interrogate specific and isolated coding behavior to reduce coding errors and ensure intended functionality, especially as code increases in complexity; unit tests also implicitly provide example usages of code. Other forms of testing are geared to discover erroneous behavior arising from unexpected inputs or emerging from the interaction of complex codebases. Although conforming to coding styles and designing tests can add time to the software development project in the short term, these foundational tools can help to improve the correctness, quality, usability, and maintainability of open-source scientific software code. They also advance the principal point of scientific research: producing accurate results in a reproducible way. In addition to suggesting several tips for getting started with clean code and testing practices, we recommend numerous tools for the popular opensource scientific software languages Python, R, and Julia.},
	pages = {e1009481},
	number = {11},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLoS} Comput Biol},
	author = {Hunter-Zinck, Haley and De Siqueira, Alexandre Fioravante and Vásquez, Váleri N. and Barnes, Richard and Martinez, Ciera C.},
	editor = {Markel, Scott},
	urldate = {2025-09-03},
	date = {2021-11-11},
	langid = {english},
}

@inproceedings{ii_test_2002,
	title = {Test reuse in the spreadsheet paradigm},
	url = {https://ieeexplore.ieee.org/abstract/document/1173265?casa_token=F0g9q2DOVv8AAAAA:ZZR8HhYjyVgyfd9JJT2v20cFms4p2ZA0DsO86Vs0DV0v_Y3bCRZUY_5SCBGtrcOb_fo70qLbJpo},
	doi = {10.1109/ISSRE.2002.1173265},
	abstract = {Spreadsheet languages are widely used by a variety of end users to perform many important tasks. Despite their perceived simplicity, spreadsheets often contain faults. Furthermore, users modify their spreadsheets frequently, which can render previously correct spreadsheets faulty. To address this problem, we previously introduced a visual approach by which users can systematically test their spreadsheets, see where new tests are required after changes, and request automated generation of potentially useful test inputs. To date, however, this approach has not taken advantage of previously developed test cases, which means that users of the approach cannot benefit, when re-testing following changes, from prior testing efforts. We have therefore been investigating ways to add support for test re-use into our spreadsheet testing methodology. In this paper we present a test re-use strategy for spreadsheets, and the algorithms that implement it, and describe their integration into our spreadsheet testing methodology. We report results of a case study examining the application of this strategy.},
	eventtitle = {13th International Symposium on Software Reliability Engineering, 2002. Proceedings.},
	pages = {257--268},
	booktitle = {13th International Symposium on Software Reliability Engineering, 2002. Proceedings.},
	author = {Ii, M.F. and Jin, D. and Rothermel, G. and Burnett, M.},
	urldate = {2025-01-27},
	date = {2002-11},
	note = {{ISSN}: 1071-9458},
	keywords = {Automatic testing, Computer science, Financial management, Pharmaceuticals, Quality assessment, Quality management, Safety, Software testing, System testing, Visualization},
}

@book{mayer_art_2022,
	location = {San Francisco},
	title = {The art of clean code: best practices to eliminate complexity and simplify your life},
	isbn = {978-1-7185-0218-5},
	shorttitle = {The art of clean code},
	pagetotal = {149},
	publisher = {No Starch Press},
	author = {Mayer, Christian},
	date = {2022},
}

@book{hunt_pragmatic_2011,
	location = {Boston},
	edition = {26. print},
	title = {The pragmatic programmer: from journeyman to master},
	isbn = {978-0-201-61622-4},
	shorttitle = {The pragmatic programmer},
	pagetotal = {321},
	publisher = {Addison-Wesley},
	author = {Hunt, Andrew and Thomas, David},
	date = {2011},
	langid = {english},
}

@inproceedings{cunha_towards_2012,
	location = {Berlin, Heidelberg},
	title = {Towards a Catalog of Spreadsheet Smells},
	isbn = {978-3-642-31128-4},
	doi = {10.1007/978-3-642-31128-4_15},
	abstract = {Spreadsheets are considered to be the most widely used programming language in the world, and reports have shown that 90\% of real-world spreadsheets contain errors.},
	pages = {202--216},
	booktitle = {Computational Science and Its Applications – {ICCSA} 2012},
	publisher = {Springer},
	author = {Cunha, Jácome and Fernandes, João P. and Ribeiro, Hugo and Saraiva, João},
	editor = {Murgante, Beniamino and Gervasi, Osvaldo and Misra, Sanjay and Nedjah, Nadia and Rocha, Ana Maria A. C. and Taniar, David and Apduhan, Bernady O.},
	date = {2012},
	langid = {english},
	keywords = {Code Smells, {EUSES} Corpus, Spreadsheets},
}

@inproceedings{chen_tree--tree_2018,
	title = {Tree-to-tree Neural Networks for Program Translation},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/d759175de8ea5b1d9a2660e45554894f-Abstract.html},
	abstract = {Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Xinyun and Liu, Chang and Song, Dawn},
	urldate = {2025-01-29},
	date = {2018},
}

@inproceedings{roziere_unsupervised_2020,
	title = {Unsupervised Translation of Programming Languages},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ed23fbf18c2cd35f8c7f8de44f85c08d-Abstract.html},
	abstract = {A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. {COBOL}, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is time-consuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source {GitHub} projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.},
	pages = {20601--20611},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Roziere, Baptiste and Lachaux, Marie-Anne and Chanussot, Lowik and Lample, Guillaume},
	urldate = {2025-01-29},
	date = {2020},
}

@article{lano_using_2024,
	title = {Using model-driven engineering to automate software language translation},
	volume = {31},
	issn = {1573-7535},
	url = {https://doi.org/10.1007/s10515-024-00419-y},
	doi = {10.1007/s10515-024-00419-y},
	abstract = {The porting or translation of software applications from one programming language to another is a common requirement of organisations that utilise software, and the increasing number and diversity of programming languages makes this capability as relevant today as in previous decades. Several approaches have been used to address this challenge, including machine learning and the manual definition of direct language-to-language translation rules, however the accuracy of these approaches remains unsatisfactory. In this paper we describe a new approach to program translation using model-driven engineering techniques: reverse-engineering source programs into specifications in the {UML} and {OCL} formalisms, and then forward-engineering the specifications to the required target language. This approach can provide assurance of semantic preservation, and additionally has the advantage of extracting precise specifications of software from code. We provide an evaluation based on a comprehensive dataset of examples, including industrial cases, and compare our results to those of other approaches and tools. Our specific contributions are: (1) Reverse-engineering source programs to detailed semantic models of software behaviour, to enable semantically-correct translations and reduce re-testing costs; (2) Program abstraction processes defined by precise and explicit rules, which can be edited and configured by users; (3) A set of reusable {OCL} library components appropriate for representing program semantics, and which can also be used for {OCL} specification of new applications; (4) A systematic procedure for building program abstractors based on language grammars and semantics.},
	pages = {20},
	number = {1},
	journaltitle = {Automated Software Engineering},
	shortjournal = {Autom Softw Eng},
	author = {Lano, Kevin and Siala, Hanan},
	urldate = {2025-01-29},
	date = {2024-02-28},
	langid = {english},
	keywords = {Artificial Intelligence, Model-driven engineering, Program translation, Re-engineering, Reverse engineering},
}

@article{ushakova_verification_2018,
	title = {Verification of Programs with Mutual Recursion in Pifagor Language},
	volume = {52},
	issn = {1558-108X},
	url = {https://doi.org/10.3103/S0146411618070301},
	doi = {10.3103/S0146411618070301},
	abstract = {In the article, we consider verification of programs with mutual recursion in the data driven functional parallel language Pifagor. In this language the program could be represented as a data flow graph, that has no control connections, and only has data relations. Under these conditions it is possible to simplify the process of formal verification, since there is no need to analyse resource conflicts, which are present in the systems with ordinary architectures. The proof of programs correctness is based on the elimination of mutual recursions by program transformation. The universal method of mutual recursion of an arbitrary number of functions elimination consists in constructing the universal recursive function that simulates all the functions in mutual recursion. A natural number is assigned to each function in mutual recursion. The universal recursive function takes as its argument the number of a function to be simulated and the arguments of this function. In some cases of the indirect recursion it is possible to use a simpler method of program transformation, namely, the merging of the functions code into a single function. To remove mutual recursion of an arbitrary number of functions, it is suggested to construct a graph of all connected functions and transform this graph by removing functions that are not connected with the target function, then by merging functions with indirect recursion and finally by constructing the universal recursive function. It is proved that in the Pifagor language such transformations of functions as code merging and universal recursive function construction do not change the correctness of the initial program. The example of partial correctness proof is given for the program that parses a simple arithmetic expression. We construct the graph of all connected functions and demonstrate two methods of proofs: by means of code merging and by means of the universal recursive function.},
	pages = {850--866},
	number = {7},
	journaltitle = {Automatic Control and Computer Sciences},
	shortjournal = {Aut. Control Comp. Sci.},
	author = {Ushakova, M. S. and Legalov, A. I.},
	urldate = {2025-01-30},
	date = {2018-12-01},
	langid = {english},
	keywords = {Pifagor programming language, correctness of recursions, data driven functional parallel programming, elimination of mutual recursion, universal recursive function},
}
