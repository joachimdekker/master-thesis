#import "../../utils/cite-tools.typ": citeauthor

= Related Work
Existing work already helps in answering the questions we set out to answer. 

== Function Extraction
#citeauthor(<lano_agile_2017>) describe a way to convert an excel application to code by extracting a UML diagram out of a spreadsheet and converting it to code, but the whole process is manual.

Object oriented models are often used to validate spreadsheets to reduce errors @engels_classsheets_2005, @cunha_spreadsheet_2015. The automated extraction of such a model is also proposed by #citeauthor(<cunha_automatically_2010>), where functional dependencies are used to detect dependencies between columns like in database normalization, and construct a model of this. They augment this with OCL to also describe the models constraints @cunha_relational_2012. In relation to this research, these works see spreadsheets as databases with relational mappings, while our research only considers the semantic computational model of Excel.

#citeauthor(<sestoft_spreadsheet_2006>) describes a full alternative implementation of spreadsheets, with subsequent master theses expanding upon the work @iversen_runtime_2006 @poulsen_optimized_2007 describe how to efficiently calculate the values in a spreadsheet using the support graph: a more organized version of the functional dependencies used in @cunha_automatically_2010. In comparison with #citeauthor(<cunha_automatically_2010>), these works also consider transitive dependencies in their model. Furthermore, they only represent the calculations, but do not generate code except for @iversen_runtime_2006. #citeauthor(<iversen_runtime_2006>) expands upon the work of #citeauthor(<sestoft_spreadsheet_2006>) by introducing runtime  code generation for speeding up parts of the calculations.

Outside the literature, few packages exist that support calling excel calculations. EPPlus @epplus_software_epplus_nodate, Espose Cells @aspose_espose_nodate, Apache POI @apache_software_apache_nodate, and SyncFusion @syncfusion_syncfusion_nodate all use their own calculation engine based on the dependencies, but do not use a support graph like in @sestoft_spreadsheet_2006. The latter also support circular dependencies but uses a different update model than in excel, where the early stopping (i.e. the definition of when the values do not differ that much) of the iterative update cannot be defined, it just updates the sheet a set amount of times.

== Validation
To verify the semantics of the program, the semantics of Excel should be defined so they can be compared with the semantics of the higher level language.

#citeauthor(<bock_semantics_2020>) defines the operational semantics for a self-made spreadsheet framework @sestoft_sheet-defined_2013 which closely reflects and closely resembles the Excel semantics. The semantics could serve as a starting point for verifying the compiler steps from Excel formulas to generated code, but need alteration because it is not clear if they fully model the excel semantics.

#citeauthor(<steinhofel_modular_2018>) describes validation of source-to-source compilation using symbolic execution, where the program is 'executed' using inference rules to create a symbolic execution tree that can be used to compare the two sources. If their symbolic execution tree is equivalent, the programs are considered equivalent @steinhofel_modular_2018. While the formal verification of the compiler is a good idea, we acknowledge that this is too big for the scope of this master project.

// Some more here?

A more manageable verification is empirical verification. #citeauthor(<rothermel_methodology_2001>) and #citeauthor(<fisher_automated_2002>) introduce the _What You See Is What You Test_ (WYSIWYT) framework for spreadsheet testing which sets it apart as emperical verification for spreadsheets. It utilizes definition-use (du) associations to link formulas to their computational or predicate uses. A spreadsheet is considered 'validated' when all du-associations are exercised by at least one test. While users can manually validate du-associations @rothermel_methodology_2001, the framework also supports generating automated test cases using random or goal-oriented approaches to satisfy all du-associations @fisher_automated_2002. This idea can be applied to our research. For a good compilation, the generated code should pass all of these automated tests to be sufficiently verified.

// Translation Validation

== Source-to-source compilation
Source to source compilation, also called transpilation, transcompilation @roziere_unsupervised_2020, or program extraction, involves the translation of one higher-level source language to a target higher-level language. The compilation should, like a normal compiler, preserve the semantics. Several techniques have been implemented.

#citeauthor(<waters_program_1988>) and #citeauthor(<ordonez_camacho_automated_2010>) both describe _transliteration and refinement_, as well as _abstraction and reimplementation_ @waters_program_1988. Transliteration involves the translation of code line-by-line to a intermediate translated representation in the target language @waters_program_1988. The intermediate representation can then be refined to better source code @waters_program_1988. @ordonez_camacho_automated_2010 translate grammars through transliteration, converting one language to the other using simple rules. They allow the user to define exceptions, which serve the same role, albeit a bit more expressive than refinement. #citeauthor(<cockx_reasonable_2022>) use simple rules to translate Agda programs into Haskell programs, essentially only doing the transliteration step. Transliteration can be applied in our research, but it mostly works with source code that is alike in grammar and semantics @waters_program_1988.

_Abstraction and reimplementation_ is a more used method @waters_program_1988 @ordonez_camacho_automated_2010 @lopes_chomsky_2005. From the source language, an intermediary representation is extracted, such as ILR @ordonez_camacho_automated_2010 or UML+OCL @lano_using_2024. The intermediary representation is then converted to the target language @waters_program_1988. This method only needs $2n$ instead of $n(n-1)$ translators @ordonez_camacho_automated_2010 @lopes_chomsky_2005. This closely relates to our research, since we will be implementing a custom domain model that models the excel 

More recently, models trained on large corpora have become a dominant approach for source-to-source compilation @yang_exploring_2024 @roziere_unsupervised_2020 @chen_tree--tree_2018. These methods leverage the ability of LLMs to generalize across tasks without specific re-training and provide guidance in their shortcomings @yang_exploring_2024. #citeauthor(<yang_exploring_2024>) propose using test case generation to provide hints to repair incorrect translations. 
However, these models require lots of annotated data @roziere_unsupervised_2020. Hence, #citeauthor(<roziere_unsupervised_2020>) proposes a way to train a model with unsupervised data, by using large corpora of open source code and applying transformations on them. #citeauthor(<guo_graphcodebert_2020>) and #citeauthor(<chen_tree--tree_2018>) propose using the AST of the source code as an additional heuristic in transforming programs to reduce errors and improve translation. Since this novel approach increases readability, it does relate to our research. However, given the lack of relevant data, we think the method will be insufficient.

== Mutual Recursion
Since the research has a focus on calculating the fixpoint of mutual recursive formulas---formulas with a circular dependency---we look at the related work in this area.

#citeauthor(<ushakova_verification_2018>) proposes a verification framework that merges multiple recursive entities into a unified recursive function. While this helps formal verification @ushakova_verification_2018, it also simplifies the generation of code, since there are fewer functions to be generated. #citeauthor(<yallop_generating_2019>) proposes a set of code generation techniques that leverage let-insertion to write mutually recursive code without scope extrusion. These techniques can be used when generating the code, but need modification since these works assume that we don't need the outcome of the individual mutual recursive formulas.

In order to calculate the results of a mutual recursion, iterative algorithms can be used. We consider a function $F: RR^N -> RR^N$ for which we need to calculate $F(arrow(x)) = arrow(x)$, where $arrow(x)$ represents the $N$ intermediate values of every cell in the mutual recursion. Calculating the fixed point can be done linearly or non-linearly @kelley_iterative_1995. The simplest approach is Picard iteration, where we simply compute the next step in the sequence $arrow(x)_(t+1) = F(arrow(x)_t)$ @kelley_iterative_1995. Several variations of this method have been developed by different researchers to reduce the number of iterations. #citeauthor(<mann_mean_1953>), #citeauthor(<ishikawa_fixed_1974>), #citeauthor(<noor_new_2000>), #citeauthor(<phuengrattana_rate_2011>), #citeauthor(<khan_picard-mann_2013>), and #citeauthor(<thakur_new_2014>) use additional functions and blending techniques to improve convergence---all demonstrated to be faster than previous methods.

The issue with most of the iterative methods described in this section is that they rely on multiple assumptions---the most significant being that they converge and are continuous. The Excel compilation must be capable of handling all inputs, including circular dependencies that do not converge or that involve conditional logic. Therefore, many of these optimizations are not applicable to our work.

== MLIR?!